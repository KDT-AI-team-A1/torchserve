2021-03-11 18:05:12,550 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retina=retina.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:05:12,556 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: retina.mar
2021-03-11 18:05:17,044 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 1ad7cb1a04f74e05afaf0ef65ab61bde
2021-03-11 18:05:17,058 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-11 18:05:17,059 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:05:17,060 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-11 18:05:17,060 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-11 18:05:17,072 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:05:17,252 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:05:17,254 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]5196
2021-03-11 18:05:17,255 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:05:17,255 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-11 18:05:17,256 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:05:17,260 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:05:17,522 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:05:17,522 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:05:17,524 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:05:17,526 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:05:17,527 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:05:17,529 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:05:19,194 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:05:19,195 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:05:19,196 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:05:19,197 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:05:19,198 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:05:19,198 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:05:19,199 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:05:19,199 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:05:19,202 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:05:19,202 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:05:19,202 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:05:19,203 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:05:19,204 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:05:19,204 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:05:19,204 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:05:19,205 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:05:19,206 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:05:19,209 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:05:19,210 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:05:20,397 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:05:20,399 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13988
2021-03-11 18:05:20,399 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:05:20,399 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:05:20,399 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:05:20,400 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:05:20,403 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:05:21,835 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:05:21,836 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:05:21,836 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:05:21,837 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:05:21,838 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:05:21,838 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:05:21,839 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:05:21,840 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:05:21,840 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:05:21,841 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:05:21,842 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:05:21,843 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:05:21,844 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:05:21,845 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:05:21,846 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:05:21,848 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:05:21,848 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:05:21,849 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:05:21,849 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:05:23,031 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:05:23,033 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]2948
2021-03-11 18:05:23,034 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:05:23,034 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:05:23,035 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:05:23,035 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:05:23,038 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:05:24,463 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:05:24,463 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:05:24,570 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:05:32,217 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:05:32,218 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:05:32,218 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:05:32,218 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:05:32,219 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:05:32,220 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:05:32,220 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:05:32,221 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:05:32,222 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:05:32,222 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:05:32,224 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:05:32,224 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:05:32,224 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-11 18:05:32,225 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:05:34,418 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:05:34,419 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]12224
2021-03-11 18:05:34,420 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:05:34,420 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:05:34,420 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:05:34,421 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:05:34,425 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:05:35,898 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:05:35,899 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:05:35,899 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:05:35,900 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:05:35,901 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:05:35,901 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:05:35,902 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:05:35,903 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:05:35,904 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:05:35,904 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:05:35,905 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:05:35,906 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:05:35,906 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:05:35,907 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:05:35,907 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:05:35,908 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:05:35,909 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2021-03-11 18:05:35,910 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:05:39,088 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:05:39,089 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]9080
2021-03-11 18:05:39,089 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:05:39,089 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:05:39,090 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:05:39,091 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:05:39,093 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:05:40,726 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:05:40,727 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:05:40,727 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:05:40,728 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:05:40,729 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:05:40,729 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:05:40,730 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:05:40,732 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:05:40,732 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:05:40,733 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:05:40,734 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:05:40,734 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:05:40,735 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:05:40,736 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:05:40,736 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:05:40,738 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:05:40,739 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2021-03-11 18:05:40,740 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:05:45,937 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:05:45,938 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]16320
2021-03-11 18:05:45,943 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:05:45,943 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:05:45,943 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:05:45,944 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:05:45,947 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:05:47,560 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:05:47,560 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:05:47,560 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:05:47,562 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:05:47,563 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:05:47,565 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:05:47,566 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:05:47,566 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:05:47,567 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:05:47,567 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:05:47,568 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:05:47,568 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:05:47,569 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2021-03-11 18:05:47,571 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:05:55,756 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:05:55,758 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]12144
2021-03-11 18:05:55,758 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:05:55,758 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:05:55,760 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:05:55,760 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:05:55,762 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:05:57,190 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:05:57,190 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:05:57,190 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:05:57,192 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:05:57,193 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:05:57,193 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:05:57,194 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:05:57,197 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:05:57,198 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:05:57,198 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:05:57,198 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:05:57,199 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:05:57,199 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:05:57,200 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:05:57,201 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:05:57,201 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:05:57,204 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2021-03-11 18:05:57,205 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:06:10,416 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:06:10,418 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]3476
2021-03-11 18:06:10,419 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:06:10,419 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:06:10,420 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:06:10,421 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:06:10,424 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:06:11,895 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:06:11,896 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:06:11,896 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:06:11,897 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:06:11,898 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:06:11,898 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:06:11,899 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:06:11,902 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:06:11,903 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:06:11,904 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:06:11,905 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:06:11,905 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:06:11,906 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:06:11,906 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:06:11,907 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:06:11,908 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:06:11,909 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2021-03-11 18:06:11,910 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:06:33,093 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:06:33,094 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]1540
2021-03-11 18:06:33,095 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:06:33,095 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:06:33,096 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:06:33,097 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:06:33,100 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:06:34,538 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:06:34,538 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:06:34,538 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:06:34,538 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:06:34,540 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:06:34,541 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:06:34,541 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:06:34,542 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:06:34,543 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:06:34,543 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:06:34,544 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:06:34,544 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:06:34,545 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:06:34,546 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:06:34,546 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:06:34,547 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:06:34,549 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2021-03-11 18:06:34,550 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:07:08,739 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:07:08,741 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10916
2021-03-11 18:07:08,742 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:07:08,742 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:07:08,742 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:07:08,744 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:07:08,746 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:07:10,181 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:07:10,182 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:07:10,182 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:07:10,184 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:07:10,184 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:07:10,184 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:07:10,185 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:07:10,187 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:07:10,187 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:07:10,188 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:07:10,188 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:07:10,189 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:07:10,190 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:07:10,190 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:07:10,191 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:07:10,191 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:07:10,193 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2021-03-11 18:07:10,194 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:07:55,683 [ERROR] nioEventLoopGroup-3-1 org.pytorch.serve.http.HttpRequestHandler - 
java.io.IOException: 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다
	at java.base/sun.nio.ch.SocketDispatcher.read0(Native Method)
	at java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276)
	at java.base/sun.nio.ch.IOUtil.read(IOUtil.java:233)
	at java.base/sun.nio.ch.IOUtil.read(IOUtil.java:223)
	at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:358)
	at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1134)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:08:05,393 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:08:05,394 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13488
2021-03-11 18:08:05,396 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:08:05,396 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:08:05,396 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:08:05,397 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:08:05,399 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:08:06,938 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:08:06,938 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:08:06,939 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:08:06,940 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:08:06,941 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:08:06,942 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:08:06,942 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:08:06,944 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:08:06,944 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:08:06,945 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:08:06,946 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:08:06,946 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:08:06,947 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:08:06,948 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:08:06,948 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:08:06,951 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:08:06,964 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2021-03-11 18:08:06,965 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:09:47,997 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: logs\config\20210311180517531-startup.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retina=retina.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:09:48,003 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20210311180517531-startup.cfg",
  "modelCount": 1,
  "created": 1615453517532,
  "models": {
    "retina": {
      "1.0": {
        "defaultVersion": true,
        "marName": "retina.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2021-03-11 18:09:48,011 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20210311180517531-startup.cfg
2021-03-11 18:09:48,013 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20210311180517531-startup.cfg validated successfully
2021-03-11 18:09:52,458 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 9e584218265b4c439caa1c828bf230bc
2021-03-11 18:09:52,471 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-11 18:09:52,471 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:09:52,472 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:09:52,473 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-11 18:09:52,474 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-11 18:09:52,487 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:09:52,669 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:09:52,671 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]11148
2021-03-11 18:09:52,672 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:09:52,672 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-11 18:09:52,672 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:09:52,677 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:09:52,932 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:09:52,933 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:09:52,937 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:09:52,937 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:09:52,937 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:09:52,939 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:09:56,206 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:09:56,206 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:09:56,207 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:09:56,208 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:09:56,208 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:09:56,209 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:09:56,210 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:09:56,210 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:09:56,211 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:09:56,211 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:09:56,212 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:09:56,212 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:09:56,213 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:09:56,213 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:09:56,214 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:09:56,215 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:09:56,215 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:09:56,216 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:09:56,216 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:09:56,217 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:09:56,217 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:09:56,219 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:09:56,220 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:09:56,224 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:09:56,234 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:09:56,246 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:09:56,246 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:09:56,247 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:09:56,250 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:09:56,251 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:09:56,252 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:09:56,252 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:09:56,253 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:09:56,254 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:09:56,254 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:09:57,431 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:09:57,433 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]12448
2021-03-11 18:09:57,433 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:09:57,434 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:09:57,434 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:09:57,434 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:09:57,436 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:09:59,574 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:10:42,055 [ERROR] nioEventLoopGroup-3-1 org.pytorch.serve.http.HttpRequestHandler - 
java.io.IOException: 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다
	at java.base/sun.nio.ch.SocketDispatcher.read0(Native Method)
	at java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276)
	at java.base/sun.nio.ch.IOUtil.read(IOUtil.java:233)
	at java.base/sun.nio.ch.IOUtil.read(IOUtil.java:223)
	at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:358)
	at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1134)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:11:50,499 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: logs\config\20210311180952940-startup.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: fastrcnn=fastrcnn.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:11:50,506 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20210311180952940-startup.cfg",
  "modelCount": 1,
  "created": 1615453792941,
  "models": {
    "retina": {
      "1.0": {
        "defaultVersion": true,
        "marName": "retina.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2021-03-11 18:11:50,514 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20210311180952940-startup.cfg
2021-03-11 18:11:50,515 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20210311180952940-startup.cfg validated successfully
2021-03-11 18:11:54,892 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag d3c1eb414c454287bbe8db44c6d3fece
2021-03-11 18:11:54,901 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-11 18:11:54,902 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:11:54,903 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:11:54,903 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-11 18:11:54,904 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-11 18:11:54,916 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:11:55,098 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:11:55,100 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]3496
2021-03-11 18:11:55,100 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:11:55,100 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-11 18:11:55,101 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:11:55,105 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:11:55,359 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:11:55,359 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:11:55,362 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:11:55,362 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:11:55,362 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:11:55,364 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:11:57,582 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:11:57,582 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:11:57,582 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:11:57,583 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:11:57,584 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:11:57,584 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:11:57,585 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:11:57,585 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:11:57,586 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:11:57,586 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:11:57,587 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:11:57,587 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:11:57,587 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:11:57,588 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:11:57,588 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:11:57,589 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:11:57,589 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:11:57,590 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:11:57,590 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:11:57,591 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:11:57,593 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:11:57,593 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:11:57,594 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:11:57,598 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:11:57,609 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:11:57,622 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:11:57,622 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:11:57,623 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:11:57,626 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:11:57,626 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:11:57,627 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:11:57,628 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:11:57,628 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:11:57,629 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:11:57,629 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:11:58,815 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:11:58,816 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]15200
2021-03-11 18:11:58,817 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:11:58,817 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:11:58,817 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:11:58,818 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:11:58,821 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:12:00,954 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:12:00,955 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:12:00,956 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:12:00,956 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:12:00,957 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:12:00,957 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:12:00,958 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:12:00,958 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:12:00,959 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:12:00,959 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:12:00,960 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:12:00,960 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:12:00,961 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:12:00,961 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:12:00,962 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:12:00,963 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:12:00,963 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:12:00,964 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:12:00,964 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:12:00,965 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:12:00,965 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:12:00,966 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:12:00,966 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:12:00,969 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:12:00,977 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:12:00,990 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:12:00,990 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:12:00,991 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:12:00,992 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:12:00,992 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:12:00,993 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:12:00,994 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:12:00,994 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:12:00,995 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:12:00,995 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:12:02,167 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:12:05,249 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]3020
2021-03-11 18:12:05,250 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:12:05,250 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:12:05,250 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:12:05,250 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:12:05,253 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:12:07,384 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:12:07,385 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:12:07,385 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:12:07,386 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:12:07,386 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:12:07,387 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:12:07,387 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:12:07,388 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:12:07,388 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:12:07,389 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:12:07,390 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:12:07,390 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:12:07,391 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:12:07,391 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:12:07,392 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:12:07,392 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:12:07,393 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:12:07,393 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:12:07,394 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:12:07,395 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:12:07,396 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:12:07,397 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:12:07,397 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:12:07,400 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:12:07,408 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:12:07,421 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:12:07,421 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:12:07,422 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:12:07,423 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:12:07,423 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:12:07,424 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:12:07,424 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:12:07,426 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-11 18:12:07,427 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:12:07,427 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:12:09,607 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:12:09,609 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]8812
2021-03-11 18:12:09,609 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:12:09,609 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:12:09,610 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:12:09,611 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:12:09,613 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:12:11,749 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:14:09,629 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 120006
2021-03-11 18:22:50,345 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:22:50,375 [ERROR] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 1
2021-03-11 18:22:50,378 [ERROR] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:198)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:22:50,379 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:22:50,380 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:22:50,380 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:22:50,381 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:22:50,382 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:22:50,382 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:22:50,382 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:22:50,382 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:22:50,382 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:22:50,397 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:22:50,413 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:22:50,418 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2021-03-11 18:22:50,419 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:22:53,608 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:22:53,609 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]2184
2021-03-11 18:22:53,610 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:22:53,610 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:22:53,612 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:22:53,612 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:22:53,614 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:22:55,744 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:22:55,745 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:22:55,747 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:22:55,748 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:22:55,748 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:22:55,748 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:22:55,749 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:22:55,749 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:22:55,750 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:22:55,750 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:22:55,751 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:22:55,751 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:22:55,752 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:22:55,752 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:22:55,753 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:22:55,753 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:22:55,754 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:22:55,754 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:22:55,755 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:22:55,755 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:22:55,756 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:22:55,756 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:22:55,757 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:22:55,759 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:22:55,769 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:22:55,782 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:22:55,782 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:22:55,783 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:22:55,784 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:22:55,785 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:22:55,785 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:22:55,786 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:22:55,786 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2021-03-11 18:22:55,787 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:22:55,787 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:23:00,966 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:23:00,968 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]780
2021-03-11 18:23:00,968 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:23:00,968 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:23:00,969 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:23:00,969 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:23:00,972 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:23:03,129 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:23:03,129 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:23:03,130 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:23:03,130 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:23:03,130 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:23:03,130 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:23:03,130 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:23:03,130 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:23:03,130 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:23:03,131 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:23:03,131 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:23:03,131 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:23:03,131 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:23:03,132 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:03,132 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:23:03,133 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:03,134 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:23:03,134 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:23:03,135 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:23:03,135 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:23:03,136 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:23:03,136 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:23:03,137 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:23:03,139 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:23:03,144 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:23:03,160 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:23:03,160 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:23:03,161 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:23:03,162 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:23:03,162 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:23:03,163 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:03,163 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:03,164 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2021-03-11 18:23:03,164 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:23:03,164 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:23:11,341 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:23:11,342 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]300
2021-03-11 18:23:11,343 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:23:11,344 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:23:11,345 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:23:11,346 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:23:11,349 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:23:13,456 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:23:13,456 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:23:13,457 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:23:13,459 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:23:13,459 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:23:13,462 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:23:13,462 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:23:13,463 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:23:13,463 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:23:13,464 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:23:13,465 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:23:13,465 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:23:13,466 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:23:13,469 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:13,469 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:23:13,470 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:13,471 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:23:13,471 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:23:13,472 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:23:13,473 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:23:13,473 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:23:13,476 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:23:13,476 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:23:13,478 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:23:13,492 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:23:13,503 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:23:13,503 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:23:13,504 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:23:13,504 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:23:13,505 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:23:13,506 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:13,506 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:13,507 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2021-03-11 18:23:13,507 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:23:13,507 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:23:13,919 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-11 18:23:13,920 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-11 18:23:13,920 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-11 18:23:15,997 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-11 18:23:16,009 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retina version 1.0
2021-03-11 18:23:16,010 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retina version: 1.0
2021-03-11 18:23:16,011 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_SCALED_DOWN
2021-03-11 18:23:16,012 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:16,013 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:16,030 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retina unregistered.
2021-03-11 18:23:26,613 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: logs\config\20210311182313921-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: fastrcnn=fastrcnn.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:23:26,620 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20210311182313921-shutdown.cfg",
  "modelCount": 1,
  "created": 1615454593921,
  "models": {
    "retina": {
      "1.0": {
        "defaultVersion": true,
        "marName": "retina.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2021-03-11 18:23:26,629 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20210311182313921-shutdown.cfg
2021-03-11 18:23:26,630 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20210311182313921-shutdown.cfg validated successfully
2021-03-11 18:23:30,962 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 40fde0f28186400bb23805abef6d49d9
2021-03-11 18:23:30,970 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-11 18:23:30,971 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:23:30,973 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:23:30,974 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-11 18:23:30,974 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-11 18:23:30,989 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:23:31,166 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:23:31,168 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10056
2021-03-11 18:23:31,169 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:23:31,169 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-11 18:23:31,170 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:23:31,175 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:23:31,432 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:23:31,433 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:23:31,434 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:23:31,434 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:23:31,435 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:23:31,436 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:23:33,607 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:23:33,608 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:23:33,609 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:23:33,610 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:23:33,611 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:23:33,612 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:23:33,613 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:23:33,614 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:23:33,615 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:23:33,616 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:23:33,616 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:23:33,617 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:23:33,617 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:23:33,618 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:33,618 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:23:33,619 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:33,620 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:23:33,620 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:23:33,622 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:23:33,622 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:23:33,623 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:23:33,623 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:23:33,624 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:23:33,628 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:23:33,634 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:23:33,651 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:23:33,652 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:23:33,653 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:23:33,657 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:23:33,659 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:23:33,659 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:33,660 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:33,661 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:23:33,662 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:23:33,662 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:23:34,839 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:23:34,841 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]7364
2021-03-11 18:23:34,841 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:23:34,841 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:23:34,843 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:23:34,843 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:23:34,846 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:23:36,936 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:23:36,937 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:23:36,938 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:23:36,940 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:23:36,940 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:23:36,941 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:23:36,942 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:23:36,943 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:23:36,943 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:23:36,943 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:23:36,944 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:23:36,945 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:23:36,946 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:23:36,946 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:36,948 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:23:36,949 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:36,949 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:23:36,950 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:23:36,950 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:23:36,951 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:23:36,952 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:23:36,953 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:23:36,956 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:23:36,959 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:23:36,969 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:23:36,984 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:23:36,985 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:23:36,986 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:23:36,989 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:23:36,990 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:23:36,990 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:36,991 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:36,992 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:23:36,993 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:23:36,993 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:23:38,183 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:23:38,184 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]16356
2021-03-11 18:23:38,185 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:23:38,185 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:23:38,187 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:23:38,187 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:23:38,193 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:23:40,284 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:23:40,285 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:23:40,286 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:23:40,287 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:23:40,288 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:23:40,289 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:23:40,289 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:23:40,290 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:23:40,290 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:23:40,291 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:23:40,292 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:23:40,293 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:23:40,293 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:23:40,295 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:40,296 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:23:40,296 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:40,296 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:23:40,297 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:23:40,297 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:23:40,298 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:23:40,299 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:23:40,300 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:23:40,300 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:23:40,304 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:23:40,313 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:23:40,327 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:23:40,328 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:23:40,328 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:23:40,330 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:23:40,331 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:23:40,332 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:40,332 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:40,333 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-11 18:23:40,334 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:23:40,334 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:23:42,518 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:23:42,519 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]1220
2021-03-11 18:23:42,520 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:23:42,520 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:23:42,521 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:23:42,521 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:23:42,526 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:23:44,615 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:23:44,615 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:23:44,619 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:23:44,619 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:23:44,620 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:23:44,620 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:23:44,620 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:23:44,621 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:23:44,622 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:23:44,622 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:23:44,623 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:23:44,624 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:23:44,624 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:23:44,625 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:44,626 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:23:44,626 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:44,627 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:23:44,628 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:23:44,628 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:23:44,629 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:23:44,629 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:23:44,630 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:23:44,630 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:23:44,633 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:23:44,643 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:23:44,656 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:23:44,657 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:23:44,657 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:23:44,659 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:23:44,659 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:23:44,660 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:44,661 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:44,661 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2021-03-11 18:23:44,662 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:23:44,662 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:23:47,850 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:23:47,851 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]12560
2021-03-11 18:23:47,852 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:23:47,852 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:23:47,853 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:23:47,854 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:23:47,857 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:23:49,952 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:23:49,952 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:23:49,953 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:23:49,954 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:23:49,955 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:23:49,956 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:23:49,957 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:23:49,957 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:23:49,958 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:23:49,958 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:23:49,961 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:23:49,962 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:23:49,962 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:23:49,963 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:49,964 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:23:49,964 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:23:49,965 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:23:49,965 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:23:49,966 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:23:49,966 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:23:49,968 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:23:49,972 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:23:49,973 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:23:49,975 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:23:49,980 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:23:49,997 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:23:49,997 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:23:49,998 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:23:49,998 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:23:50,000 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:23:50,004 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:50,004 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:50,006 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2021-03-11 18:23:50,006 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:23:50,006 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:23:50,411 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-11 18:23:50,411 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-11 18:23:50,411 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-11 18:23:52,477 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-11 18:23:52,491 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retina version 1.0
2021-03-11 18:23:52,491 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retina version: 1.0
2021-03-11 18:23:52,492 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_SCALED_DOWN
2021-03-11 18:23:52,493 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:23:52,494 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:23:52,509 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retina unregistered.
2021-03-11 18:25:35,986 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: fastrcnn=fastrcnn.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:25:36,015 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: fastrcnn.mar
2021-03-11 18:25:40,861 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 7088e817d9734172bf40b287a23d442b
2021-03-11 18:25:40,877 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model fastrcnn
2021-03-11 18:25:40,878 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model fastrcnn
2021-03-11 18:25:40,878 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model fastrcnn loaded.
2021-03-11 18:25:40,878 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: fastrcnn, count: 1
2021-03-11 18:25:40,892 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:25:41,078 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:25:41,080 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]15952
2021-03-11 18:25:41,081 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:25:41,082 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fastrcnn_1.0 State change null -> WORKER_STARTED
2021-03-11 18:25:41,083 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:25:41,087 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:25:41,341 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:25:41,342 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:25:41,344 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:25:41,346 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:25:41,347 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:25:41,349 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:25:43,625 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2248
2021-03-11 18:25:43,625 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fastrcnn_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2021-03-11 18:26:48,909 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-11 18:26:48,910 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-11 18:26:48,910 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-11 18:26:50,968 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-11 18:26:50,998 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model fastrcnn version 1.0
2021-03-11 18:26:50,998 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: fastrcnn version: 1.0
2021-03-11 18:26:50,999 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-fastrcnn_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2021-03-11 18:26:51,000 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fastrcnn_1.0-stderr
2021-03-11 18:26:51,000 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2021-03-11 18:26:51,000 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Frontend disconnected.
2021-03-11 18:26:51,001 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fastrcnn_1.0-stdout
2021-03-11 18:26:51,004 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2021-03-11 18:26:51,006 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2021-03-11 18:26:51,007 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fastrcnn_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2021-03-11 18:26:51,008 [WARN ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fastrcnn_1.0-stderr
2021-03-11 18:26:51,009 [WARN ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fastrcnn_1.0-stdout
2021-03-11 18:26:51,009 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2021-03-11 18:26:51,011 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fastrcnn_1.0-stdout
2021-03-11 18:26:51,011 [INFO ] W-9000-fastrcnn_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fastrcnn_1.0-stderr
2021-03-11 18:26:51,130 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model fastrcnn unregistered.
2021-03-11 18:27:02,057 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retina=retina.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:27:02,085 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: retina.mar
2021-03-11 18:27:06,475 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 01f2c0bbb65747ee8329f8c80bf23d86
2021-03-11 18:27:06,489 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-11 18:27:06,489 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:27:06,491 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-11 18:27:06,492 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-11 18:27:06,505 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:27:06,686 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:27:06,687 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]8944
2021-03-11 18:27:06,688 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:27:06,689 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-11 18:27:06,690 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:27:06,694 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:27:06,951 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:27:06,952 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:27:06,954 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:27:06,956 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:27:06,957 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:27:06,959 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:27:09,129 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:27:09,130 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:27:09,132 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:27:09,133 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:27:09,133 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:27:09,134 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:27:09,134 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:27:09,135 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:27:09,136 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:27:09,136 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:27:09,137 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:27:09,138 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:27:09,138 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:27:09,140 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:27:09,141 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:27:09,141 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:27:09,142 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:27:09,143 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:27:09,143 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:27:09,144 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:27:09,145 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:27:09,145 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:27:09,147 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:27:09,151 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:27:09,160 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:27:09,174 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:27:09,174 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:27:09,175 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:27:09,177 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:27:09,177 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:27:09,178 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:27:09,179 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:27:09,180 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:27:09,181 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:27:09,181 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:27:10,357 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:27:10,359 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]5252
2021-03-11 18:27:10,362 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:27:10,362 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:27:10,362 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:27:10,363 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:27:10,366 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:27:12,475 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:27:12,475 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:27:12,476 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:27:12,478 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:27:12,479 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:27:12,480 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:27:12,480 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:27:12,481 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:27:12,481 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:27:12,482 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:27:12,483 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:27:12,483 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:27:12,484 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:27:12,485 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:27:12,485 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:27:12,487 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:27:12,487 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:27:12,488 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:27:12,489 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:27:12,489 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:27:12,490 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:27:12,490 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:27:12,491 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:27:12,497 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:27:12,505 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:27:12,522 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:27:12,522 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:27:12,523 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:27:12,524 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:27:12,526 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:27:12,527 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:27:12,527 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:27:12,529 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:27:12,529 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:27:12,529 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:27:13,707 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:27:13,708 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]2856
2021-03-11 18:27:13,709 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:27:13,709 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:27:13,711 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:27:13,711 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:27:13,714 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:27:15,842 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:27:15,842 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:27:15,844 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:27:15,846 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:27:15,846 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:27:15,847 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:27:15,849 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:27:15,849 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:27:15,850 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:27:15,851 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:27:15,851 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:27:15,852 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:27:15,853 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:27:15,853 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:27:15,856 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:27:15,856 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:27:15,857 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:27:15,858 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:27:15,859 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:27:15,859 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:27:15,860 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:27:15,861 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:27:15,862 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:27:15,865 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:27:15,871 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-11 18:27:15,886 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:27:15,887 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:27:15,888 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:27:15,889 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:27:15,891 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:27:15,892 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:27:15,892 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:27:15,894 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-11 18:27:15,895 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:27:15,895 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:27:17,578 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-11 18:27:17,579 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-11 18:27:17,583 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-11 18:27:18,078 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:27:18,080 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]9976
2021-03-11 18:27:18,080 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:27:18,080 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:27:18,080 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:27:18,081 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:27:18,083 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:27:19,655 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-11 18:27:19,666 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retina version 1.0
2021-03-11 18:27:20,247 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:27:22,462 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:27:22,462 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retina version: 1.0
2021-03-11 18:27:22,462 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:27:22,462 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_SCALED_DOWN
2021-03-11 18:27:22,463 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:27:22,464 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:27:22,464 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:27:22,464 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2021-03-11 18:27:22,464 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:27:22,464 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:27:22,469 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2021-03-11 18:27:22,469 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:27:22,469 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2021-03-11 18:27:22,470 [WARN ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - --- Logging error ---
2021-03-11 18:27:22,471 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:27:22,472 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:27:22,472 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2021-03-11 18:27:22,477 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:27:22,481 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:27:22,481 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2021-03-11 18:27:22,583 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retina unregistered.
2021-03-11 18:45:51,383 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retina=retina.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:45:51,413 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: retina.mar
2021-03-11 18:45:55,792 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 7b41839d720a4f749848330f62c54bfd
2021-03-11 18:45:55,806 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-11 18:45:55,806 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:45:55,806 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-11 18:45:55,807 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-11 18:45:55,819 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:45:55,996 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:45:55,997 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10320
2021-03-11 18:45:55,998 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:45:55,998 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:45:55,998 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-11 18:45:56,002 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:45:56,263 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:45:56,263 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:45:56,265 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:45:56,266 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:45:56,266 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:45:56,267 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:45:58,545 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:45:58,545 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:45:58,545 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:45:58,546 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:45:58,546 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:45:58,546 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:45:58,546 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:45:58,546 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:45:58,547 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:45:58,547 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:45:58,547 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:45:58,548 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:45:58,548 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:45:58,549 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:45:58,549 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:45:58,550 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:45:58,550 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:45:58,551 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:45:58,551 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:45:58,552 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:45:58,552 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:45:58,553 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:45:58,553 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:45:58,557 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:45:58,567 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "backbone.fpn_lateral3.weight", "backbone.fpn_lateral3.bias", "backbone.fpn_output3.weight", "backbone.fpn_output3.bias", "backbone.fpn_lateral4.weight", "backbone.fpn_lateral4.bias", "backbone.fpn_output4.weight", "backbone.fpn_output4.bias", "backbone.fpn_lateral5.weight", "backbone.fpn_lateral5.bias", "backbone.fpn_output5.weight", "backbone.fpn_output5.bias", "backbone.top_block.p6.weight", "backbone.top_block.p6.bias", "backbone.top_block.p7.weight", "backbone.top_block.p7.bias", "backbone.bottom_up.stem.conv1.weight", "backbone.bottom_up.stem.conv1.norm.weight", "backbone.bottom_up.stem.conv1.norm.bias", "backbone.bottom_up.stem.conv1.norm.running_mean", "backbone.bottom_up.stem.conv1.norm.running_var", "backbone.bottom_up.res2.0.shortcut.weight", "backbone.bottom_up.res2.0.shortcut.norm.weight", "backbone.bottom_up.res2.0.shortcut.norm.bias", "backbone.bottom_up.res2.0.shortcut.norm.running_mean", "backbone.bottom_up.res2.0.shortcut.norm.running_var", "backbone.bottom_up.res2.0.conv1.weight", "backbone.bottom_up.res2.0.conv1.norm.weight", "backbone.bottom_up.res2.0.conv1.norm.bias", "backbone.bottom_up.res2.0.conv1.norm.running_mean", "backbone.bottom_up.res2.0.conv1.norm.running_var", "backbone.bottom_up.res2.0.conv2.weight", "backbone.bottom_up.res2.0.conv2.norm.weight", "backbone.bottom_up.res2.0.conv2.norm.bias", "backbone.bottom_up.res2.0.conv2.norm.running_mean", "backbone.bottom_up.res2.0.conv2.norm.running_var", "backbone.bottom_up.res2.0.conv3.weight", "backbone.bottom_up.res2.0.conv3.norm.weight", "backbone.bottom_up.res2.0.conv3.norm.bias", "backbone.bottom_up.res2.0.conv3.norm.running_mean", "backbone.bottom_up.res2.0.conv3.norm.running_var", "backbone.bottom_up.res2.1.conv1.weight", "backbone.bottom_up.res2.1.conv1.norm.weight", "backbone.bottom_up.res2.1.conv1.norm.bias", "backbone.bottom_up.res2.1.conv1.norm.running_mean", "backbone.bottom_up.res2.1.conv1.norm.running_var", "backbone.bottom_up.res2.1.conv2.weight", "backbone.bottom_up.res2.1.conv2.norm.weight", "backbone.bottom_up.res2.1.conv2.norm.bias", "backbone.bottom_up.res2.1.conv2.norm.running_mean", "backbone.bottom_up.res2.1.conv2.norm.running_var", "backbone.bottom_up.res2.1.conv3.weight", "backbone.bottom_up.res2.1.conv3.norm.weight", "backbone.bottom_up.res2.1.conv3.norm.bias", "backbone.bottom_up.res2.1.conv3.norm.running_mean", "backbone.bottom_up.res2.1.conv3.norm.running_var", "backbone.bottom_up.res2.2.conv1.weight", "backbone.bottom_up.res2.2.conv1.norm.weight", "backbone.bottom_up.res2.2.conv1.norm.bias", "backbone.bottom_up.res2.2.conv1.norm.running_mean", "backbone.bottom_up.res2.2.conv1.norm.running_var", "backbone.bottom_up.res2.2.conv2.weight", "backbone.bottom_up.res2.2.conv2.norm.weight", "backbone.bottom_up.res2.2.conv2.norm.bias", "backbone.bottom_up.res2.2.conv2.norm.running_mean", "backbone.bottom_up.res2.2.conv2.norm.running_var", "backbone.bottom_up.res2.2.conv3.weight", "backbone.bottom_up.res2.2.conv3.norm.weight", "backbone.bottom_up.res2.2.conv3.norm.bias", "backbone.bottom_up.res2.2.conv3.norm.running_mean", "backbone.bottom_up.res2.2.conv3.norm.running_var", "backbone.bottom_up.res3.0.shortcut.weight", "backbone.bottom_up.res3.0.shortcut.norm.weight", "backbone.bottom_up.res3.0.shortcut.norm.bias", "backbone.bottom_up.res3.0.shortcut.norm.running_mean", "backbone.bottom_up.res3.0.shortcut.norm.running_var", "backbone.bottom_up.res3.0.conv1.weight", "backbone.bottom_up.res3.0.conv1.norm.weight", "backbone.bottom_up.res3.0.conv1.norm.bias", "backbone.bottom_up.res3.0.conv1.norm.running_mean", "backbone.bottom_up.res3.0.conv1.norm.running_var", "backbone.bottom_up.res3.0.conv2.weight", "backbone.bottom_up.res3.0.conv2.norm.weight", "backbone.bottom_up.res3.0.conv2.norm.bias", "backbone.bottom_up.res3.0.conv2.norm.running_mean", "backbone.bottom_up.res3.0.conv2.norm.running_var", "backbone.bottom_up.res3.0.conv3.weight", "backbone.bottom_up.res3.0.conv3.norm.weight", "backbone.bottom_up.res3.0.conv3.norm.bias", "backbone.bottom_up.res3.0.conv3.norm.running_mean", "backbone.bottom_up.res3.0.conv3.norm.running_var", "backbone.bottom_up.res3.1.conv1.weight", "backbone.bottom_up.res3.1.conv1.norm.weight", "backbone.bottom_up.res3.1.conv1.norm.bias", "backbone.bottom_up.res3.1.conv1.norm.running_mean", "backbone.bottom_up.res3.1.conv1.norm.running_var", "backbone.bottom_up.res3.1.conv2.weight", "backbone.bottom_up.res3.1.conv2.norm.weight", "backbone.bottom_up.res3.1.conv2.norm.bias", "backbone.bottom_up.res3.1.conv2.norm.running_mean", "backbone.bottom_up.res3.1.conv2.norm.running_var", "backbone.bottom_up.res3.1.conv3.weight", "backbone.bottom_up.res3.1.conv3.norm.weight", "backbone.bottom_up.res3.1.conv3.norm.bias", "backbone.bottom_up.res3.1.conv3.norm.running_mean", "backbone.bottom_up.res3.1.conv3.norm.running_var", "backbone.bottom_up.res3.2.conv1.weight", "backbone.bottom_up.res3.2.conv1.norm.weight", "backbone.bottom_up.res3.2.conv1.norm.bias", "backbone.bottom_up.res3.2.conv1.norm.running_mean", "backbone.bottom_up.res3.2.conv1.norm.running_var", "backbone.bottom_up.res3.2.conv2.weight", "backbone.bottom_up.res3.2.conv2.norm.weight", "backbone.bottom_up.res3.2.conv2.norm.bias", "backbone.bottom_up.res3.2.conv2.norm.running_mean", "backbone.bottom_up.res3.2.conv2.norm.running_var", "backbone.bottom_up.res3.2.conv3.weight", "backbone.bottom_up.res3.2.conv3.norm.weight", "backbone.bottom_up.res3.2.conv3.norm.bias", "backbone.bottom_up.res3.2.conv3.norm.running_mean", "backbone.bottom_up.res3.2.conv3.norm.running_var", "backbone.bottom_up.res3.3.conv1.weight", "backbone.bottom_up.res3.3.conv1.norm.weight", "backbone.bottom_up.res3.3.conv1.norm.bias", "backbone.bottom_up.res3.3.conv1.norm.running_mean", "backbone.bottom_up.res3.3.conv1.norm.running_var", "backbone.bottom_up.res3.3.conv2.weight", "backbone.bottom_up.res3.3.conv2.norm.weight", "backbone.bottom_up.res3.3.conv2.norm.bias", "backbone.bottom_up.res3.3.conv2.norm.running_mean", "backbone.bottom_up.res3.3.conv2.norm.running_var", "backbone.bottom_up.res3.3.conv3.weight", "backbone.bottom_up.res3.3.conv3.norm.weight", "backbone.bottom_up.res3.3.conv3.norm.bias", "backbone.bottom_up.res3.3.conv3.norm.running_mean", "backbone.bottom_up.res3.3.conv3.norm.running_var", "backbone.bottom_up.res4.0.shortcut.weight", "backbone.bottom_up.res4.0.shortcut.norm.weight", "backbone.bottom_up.res4.0.shortcut.norm.bias", "backbone.bottom_up.res4.0.shortcut.norm.running_mean", "backbone.bottom_up.res4.0.shortcut.norm.running_var", "backbone.bottom_up.res4.0.conv1.weight", "backbone.bottom_up.res4.0.conv1.norm.weight", "backbone.bottom_up.res4.0.conv1.norm.bias", "backbone.bottom_up.res4.0.conv1.norm.running_mean", "backbone.bottom_up.res4.0.conv1.norm.running_var", "backbone.bottom_up.res4.0.conv2.weight", "backbone.bottom_up.res4.0.conv2.norm.weight", "backbone.bottom_up.res4.0.conv2.norm.bias", "backbone.bottom_up.res4.0.conv2.norm.running_mean", "backbone.bottom_up.res4.0.conv2.norm.running_var", "backbone.bottom_up.res4.0.conv3.weight", "backbone.bottom_up.res4.0.conv3.norm.weight", "backbone.bottom_up.res4.0.conv3.norm.bias", "backbone.bottom_up.res4.0.conv3.norm.running_mean", "backbone.bottom_up.res4.0.conv3.norm.running_var", "backbone.bottom_up.res4.1.conv1.weight", "backbone.bottom_up.res4.1.conv1.norm.weight", "backbone.bottom_up.res4.1.conv1.norm.bias", "backbone.bottom_up.res4.1.conv1.norm.running_mean", "backbone.bottom_up.res4.1.conv1.norm.running_var", "backbone.bottom_up.res4.1.conv2.weight", "backbone.bottom_up.res4.1.conv2.norm.weight", "backbone.bottom_up.res4.1.conv2.norm.bias", "backbone.bottom_up.res4.1.conv2.norm.running_mean", "backbone.bottom_up.res4.1.conv2.norm.running_var", "backbone.bottom_up.res4.1.conv3.weight", "backbone.bottom_up.res4.1.conv3.norm.weight", "backbone.bottom_up.res4.1.conv3.norm.bias", "backbone.bottom_up.res4.1.conv3.norm.running_mean", "backbone.bottom_up.res4.1.conv3.norm.running_var", "backbone.bottom_up.res4.2.conv1.weight", "backbone.bottom_up.res4.2.conv1.norm.weight", "backbone.bottom_up.res4.2.conv1.norm.bias", "backbone.bottom_up.res4.2.conv1.norm.running_mean", "backbone.bottom_up.res4.2.conv1.norm.running_var", "backbone.bottom_up.res4.2.conv2.weight", "backbone.bottom_up.res4.2.conv2.norm.weight", "backbone.bottom_up.res4.2.conv2.norm.bias", "backbone.bottom_up.res4.2.conv2.norm.running_mean", "backbone.bottom_up.res4.2.conv2.norm.running_var", "backbone.bottom_up.res4.2.conv3.weight", "backbone.bottom_up.res4.2.conv3.norm.weight", "backbone.bottom_up.res4.2.conv3.norm.bias", "backbone.bottom_up.res4.2.conv3.norm.running_mean", "backbone.bottom_up.res4.2.conv3.norm.running_var", "backbone.bottom_up.res4.3.conv1.weight", "backbone.bottom_up.res4.3.conv1.norm.weight", "backbone.bottom_up.res4.3.conv1.norm.bias", "backbone.bottom_up.res4.3.conv1.norm.running_mean", "backbone.bottom_up.res4.3.conv1.norm.running_var", "backbone.bottom_up.res4.3.conv2.weight", "backbone.bottom_up.res4.3.conv2.norm.weight", "backbone.bottom_up.res4.3.conv2.norm.bias", "backbone.bottom_up.res4.3.conv2.norm.running_mean", "backbone.bottom_up.res4.3.conv2.norm.running_var", "backbone.bottom_up.res4.3.conv3.weight", "backbone.bottom_up.res4.3.conv3.norm.weight", "backbone.bottom_up.res4.3.conv3.norm.bias", "backbone.bottom_up.res4.3.conv3.norm.running_mean", "backbone.bottom_up.res4.3.conv3.norm.running_var", "backbone.bottom_up.res4.4.conv1.weight", "backbone.bottom_up.res4.4.conv1.norm.weight", "backbone.bottom_up.res4.4.conv1.norm.bias", "backbone.bottom_up.res4.4.conv1.norm.running_mean", "backbone.bottom_up.res4.4.conv1.norm.running_var", "backbone.bottom_up.res4.4.conv2.weight", "backbone.bottom_up.res4.4.conv2.norm.weight", "backbone.bottom_up.res4.4.conv2.norm.bias", "backbone.bottom_up.res4.4.conv2.norm.running_mean", "backbone.bottom_up.res4.4.conv2.norm.running_var", "backbone.bottom_up.res4.4.conv3.weight", "backbone.bottom_up.res4.4.conv3.norm.weight", "backbone.bottom_up.res4.4.conv3.norm.bias", "backbone.bottom_up.res4.4.conv3.norm.running_mean", "backbone.bottom_up.res4.4.conv3.norm.running_var", "backbone.bottom_up.res4.5.conv1.weight", "backbone.bottom_up.res4.5.conv1.norm.weight", "backbone.bottom_up.res4.5.conv1.norm.bias", "backbone.bottom_up.res4.5.conv1.norm.running_mean", "backbone.bottom_up.res4.5.conv1.norm.running_var", "backbone.bottom_up.res4.5.conv2.weight", "backbone.bottom_up.res4.5.conv2.norm.weight", "backbone.bottom_up.res4.5.conv2.norm.bias", "backbone.bottom_up.res4.5.conv2.norm.running_mean", "backbone.bottom_up.res4.5.conv2.norm.running_var", "backbone.bottom_up.res4.5.conv3.weight", "backbone.bottom_up.res4.5.conv3.norm.weight", "backbone.bottom_up.res4.5.conv3.norm.bias", "backbone.bottom_up.res4.5.conv3.norm.running_mean", "backbone.bottom_up.res4.5.conv3.norm.running_var", "backbone.bottom_up.res5.0.shortcut.weight", "backbone.bottom_up.res5.0.shortcut.norm.weight", "backbone.bottom_up.res5.0.shortcut.norm.bias", "backbone.bottom_up.res5.0.shortcut.norm.running_mean", "backbone.bottom_up.res5.0.shortcut.norm.running_var", "backbone.bottom_up.res5.0.conv1.weight", "backbone.bottom_up.res5.0.conv1.norm.weight", "backbone.bottom_up.res5.0.conv1.norm.bias", "backbone.bottom_up.res5.0.conv1.norm.running_mean", "backbone.bottom_up.res5.0.conv1.norm.running_var", "backbone.bottom_up.res5.0.conv2.weight", "backbone.bottom_up.res5.0.conv2.norm.weight", "backbone.bottom_up.res5.0.conv2.norm.bias", "backbone.bottom_up.res5.0.conv2.norm.running_mean", "backbone.bottom_up.res5.0.conv2.norm.running_var", "backbone.bottom_up.res5.0.conv3.weight", "backbone.bottom_up.res5.0.conv3.norm.weight", "backbone.bottom_up.res5.0.conv3.norm.bias", "backbone.bottom_up.res5.0.conv3.norm.running_mean", "backbone.bottom_up.res5.0.conv3.norm.running_var", "backbone.bottom_up.res5.1.conv1.weight", "backbone.bottom_up.res5.1.conv1.norm.weight", "backbone.bottom_up.res5.1.conv1.norm.bias", "backbone.bottom_up.res5.1.conv1.norm.running_mean", "backbone.bottom_up.res5.1.conv1.norm.running_var", "backbone.bottom_up.res5.1.conv2.weight", "backbone.bottom_up.res5.1.conv2.norm.weight", "backbone.bottom_up.res5.1.conv2.norm.bias", "backbone.bottom_up.res5.1.conv2.norm.running_mean", "backbone.bottom_up.res5.1.conv2.norm.running_var", "backbone.bottom_up.res5.1.conv3.weight", "backbone.bottom_up.res5.1.conv3.norm.weight", "backbone.bottom_up.res5.1.conv3.norm.bias", "backbone.bottom_up.res5.1.conv3.norm.running_mean", "backbone.bottom_up.res5.1.conv3.norm.running_var", "backbone.bottom_up.res5.2.conv1.weight", "backbone.bottom_up.res5.2.conv1.norm.weight", "backbone.bottom_up.res5.2.conv1.norm.bias", "backbone.bottom_up.res5.2.conv1.norm.running_mean", "backbone.bottom_up.res5.2.conv1.norm.running_var", "backbone.bottom_up.res5.2.conv2.weight", "backbone.bottom_up.res5.2.conv2.norm.weight", "backbone.bottom_up.res5.2.conv2.norm.bias", "backbone.bottom_up.res5.2.conv2.norm.running_mean", "backbone.bottom_up.res5.2.conv2.norm.running_var", "backbone.bottom_up.res5.2.conv3.weight", "backbone.bottom_up.res5.2.conv3.norm.weight", "backbone.bottom_up.res5.2.conv3.norm.bias", "backbone.bottom_up.res5.2.conv3.norm.running_mean", "backbone.bottom_up.res5.2.conv3.norm.running_var", "anchor_generator.cell_anchors.0", "anchor_generator.cell_anchors.1", "anchor_generator.cell_anchors.2", "anchor_generator.cell_anchors.3", "anchor_generator.cell_anchors.4", "head.cls_subnet.0.weight", "head.cls_subnet.0.bias", "head.cls_subnet.2.weight", "head.cls_subnet.2.bias", "head.cls_subnet.4.weight", "head.cls_subnet.4.bias", "head.cls_subnet.6.weight", "head.cls_subnet.6.bias", "head.bbox_subnet.0.weight", "head.bbox_subnet.0.bias", "head.bbox_subnet.2.weight", "head.bbox_subnet.2.bias", "head.bbox_subnet.4.weight", "head.bbox_subnet.4.bias", "head.bbox_subnet.6.weight", "head.bbox_subnet.6.bias", "head.cls_score.weight", "head.cls_score.bias", "head.bbox_pred.weight", "head.bbox_pred.bias". 
2021-03-11 18:45:58,591 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:45:58,591 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:45:58,592 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:45:58,595 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:45:58,595 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:45:58,596 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:45:58,597 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:45:58,598 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:45:58,598 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:45:58,598 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:45:59,303 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-11 18:45:59,303 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-11 18:45:59,304 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-11 18:45:59,786 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:45:59,788 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]12668
2021-03-11 18:45:59,788 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:45:59,788 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:45:59,789 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:45:59,789 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:45:59,793 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:46:01,370 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-11 18:46:01,383 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retina version 1.0
2021-03-11 18:46:01,383 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retina version: 1.0
2021-03-11 18:46:01,385 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_SCALED_DOWN
2021-03-11 18:46:01,386 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:46:01,386 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2021-03-11 18:46:01,386 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:46:01,387 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2021-03-11 18:46:01,389 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2021-03-11 18:46:01,390 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:46:01,390 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2021-03-11 18:46:01,391 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:46:01,391 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:46:01,392 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2021-03-11 18:46:01,393 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:46:01,393 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:46:01,511 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retina unregistered.
2021-03-11 18:48:59,021 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: mnist=mnist.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:48:59,048 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: mnist.mar
2021-03-11 18:48:59,245 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 20be6a4381ab4678a1d0656af9b95694
2021-03-11 18:48:59,265 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2021-03-11 18:48:59,265 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2021-03-11 18:48:59,266 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2021-03-11 18:48:59,267 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2021-03-11 18:48:59,280 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:48:59,461 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:48:59,463 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]9208
2021-03-11 18:48:59,465 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:48:59,465 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2021-03-11 18:48:59,466 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:48:59,471 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:48:59,732 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:48:59,732 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:48:59,734 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:48:59,736 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:48:59,737 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:48:59,738 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:49:01,592 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Missing the index_to_name.json file. Inference output will not include class name.
2021-03-11 18:49:01,597 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1830
2021-03-11 18:49:01,598 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2021-03-11 18:49:47,546 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Calculating Explanations
2021-03-11 18:49:47,546 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Getting data and target
2021-03-11 18:49:47,599 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - input shape torch.Size([1, 1, 28, 28])
2021-03-11 18:49:47,599 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2021-03-11 18:49:47,603 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.job.Job - Waiting time ns: 137500, Backend time ns: 70752500
2021-03-11 18:51:26,961 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Calculating Explanations
2021-03-11 18:51:26,962 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Getting data and target
2021-03-11 18:51:26,977 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 17
2021-03-11 18:51:26,977 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - input shape torch.Size([1, 1, 28, 28])
2021-03-11 18:51:26,981 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.job.Job - Waiting time ns: 74000, Backend time ns: 20531800
2021-03-11 18:52:18,820 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-11 18:52:18,821 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-11 18:52:18,821 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-11 18:52:20,896 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-11 18:52:20,907 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model mnist version 1.0
2021-03-11 18:52:20,907 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: mnist version: 1.0
2021-03-11 18:52:20,908 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2021-03-11 18:52:20,908 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2021-03-11 18:52:20,909 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2021-03-11 18:52:20,909 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Frontend disconnected.
2021-03-11 18:52:20,910 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2021-03-11 18:52:20,915 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2021-03-11 18:52:20,916 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2021-03-11 18:52:20,918 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2021-03-11 18:52:20,918 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2021-03-11 18:52:20,920 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2021-03-11 18:52:20,921 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2021-03-11 18:52:20,921 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2021-03-11 18:52:20,922 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2021-03-11 18:52:21,030 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model mnist unregistered.
2021-03-11 18:55:30,002 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retina=retina.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-11 18:55:30,030 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: retina.mar
2021-03-11 18:55:34,544 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag bae4d13497de4ac5b867b4182e7ae228
2021-03-11 18:55:34,556 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-11 18:55:34,557 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-11 18:55:34,557 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-11 18:55:34,558 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-11 18:55:34,569 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-11 18:55:34,752 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:55:34,754 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]11536
2021-03-11 18:55:34,754 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:55:34,754 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:55:34,754 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-11 18:55:34,758 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:55:35,011 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-11 18:55:35,011 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-11 18:55:35,012 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:55:35,016 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-11 18:55:35,016 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-11 18:55:35,017 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-11 18:55:37,236 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:55:37,236 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:55:37,238 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:55:37,239 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:55:37,240 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:55:37,241 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:55:37,241 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:55:37,244 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:55:37,245 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:55:37,245 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:55:37,246 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:55:37,247 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:55:37,247 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:55:37,248 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:55:37,249 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:55:37,251 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:55:37,251 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:55:37,252 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:55:37,253 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:55:37,254 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:55:37,254 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:55:37,255 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:55:37,256 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:55:37,261 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:55:37,270 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "backbone.fpn_lateral3.weight", "backbone.fpn_lateral3.bias", "backbone.fpn_output3.weight", "backbone.fpn_output3.bias", "backbone.fpn_lateral4.weight", "backbone.fpn_lateral4.bias", "backbone.fpn_output4.weight", "backbone.fpn_output4.bias", "backbone.fpn_lateral5.weight", "backbone.fpn_lateral5.bias", "backbone.fpn_output5.weight", "backbone.fpn_output5.bias", "backbone.top_block.p6.weight", "backbone.top_block.p6.bias", "backbone.top_block.p7.weight", "backbone.top_block.p7.bias", "backbone.bottom_up.stem.conv1.weight", "backbone.bottom_up.stem.conv1.norm.weight", "backbone.bottom_up.stem.conv1.norm.bias", "backbone.bottom_up.stem.conv1.norm.running_mean", "backbone.bottom_up.stem.conv1.norm.running_var", "backbone.bottom_up.res2.0.shortcut.weight", "backbone.bottom_up.res2.0.shortcut.norm.weight", "backbone.bottom_up.res2.0.shortcut.norm.bias", "backbone.bottom_up.res2.0.shortcut.norm.running_mean", "backbone.bottom_up.res2.0.shortcut.norm.running_var", "backbone.bottom_up.res2.0.conv1.weight", "backbone.bottom_up.res2.0.conv1.norm.weight", "backbone.bottom_up.res2.0.conv1.norm.bias", "backbone.bottom_up.res2.0.conv1.norm.running_mean", "backbone.bottom_up.res2.0.conv1.norm.running_var", "backbone.bottom_up.res2.0.conv2.weight", "backbone.bottom_up.res2.0.conv2.norm.weight", "backbone.bottom_up.res2.0.conv2.norm.bias", "backbone.bottom_up.res2.0.conv2.norm.running_mean", "backbone.bottom_up.res2.0.conv2.norm.running_var", "backbone.bottom_up.res2.0.conv3.weight", "backbone.bottom_up.res2.0.conv3.norm.weight", "backbone.bottom_up.res2.0.conv3.norm.bias", "backbone.bottom_up.res2.0.conv3.norm.running_mean", "backbone.bottom_up.res2.0.conv3.norm.running_var", "backbone.bottom_up.res2.1.conv1.weight", "backbone.bottom_up.res2.1.conv1.norm.weight", "backbone.bottom_up.res2.1.conv1.norm.bias", "backbone.bottom_up.res2.1.conv1.norm.running_mean", "backbone.bottom_up.res2.1.conv1.norm.running_var", "backbone.bottom_up.res2.1.conv2.weight", "backbone.bottom_up.res2.1.conv2.norm.weight", "backbone.bottom_up.res2.1.conv2.norm.bias", "backbone.bottom_up.res2.1.conv2.norm.running_mean", "backbone.bottom_up.res2.1.conv2.norm.running_var", "backbone.bottom_up.res2.1.conv3.weight", "backbone.bottom_up.res2.1.conv3.norm.weight", "backbone.bottom_up.res2.1.conv3.norm.bias", "backbone.bottom_up.res2.1.conv3.norm.running_mean", "backbone.bottom_up.res2.1.conv3.norm.running_var", "backbone.bottom_up.res2.2.conv1.weight", "backbone.bottom_up.res2.2.conv1.norm.weight", "backbone.bottom_up.res2.2.conv1.norm.bias", "backbone.bottom_up.res2.2.conv1.norm.running_mean", "backbone.bottom_up.res2.2.conv1.norm.running_var", "backbone.bottom_up.res2.2.conv2.weight", "backbone.bottom_up.res2.2.conv2.norm.weight", "backbone.bottom_up.res2.2.conv2.norm.bias", "backbone.bottom_up.res2.2.conv2.norm.running_mean", "backbone.bottom_up.res2.2.conv2.norm.running_var", "backbone.bottom_up.res2.2.conv3.weight", "backbone.bottom_up.res2.2.conv3.norm.weight", "backbone.bottom_up.res2.2.conv3.norm.bias", "backbone.bottom_up.res2.2.conv3.norm.running_mean", "backbone.bottom_up.res2.2.conv3.norm.running_var", "backbone.bottom_up.res3.0.shortcut.weight", "backbone.bottom_up.res3.0.shortcut.norm.weight", "backbone.bottom_up.res3.0.shortcut.norm.bias", "backbone.bottom_up.res3.0.shortcut.norm.running_mean", "backbone.bottom_up.res3.0.shortcut.norm.running_var", "backbone.bottom_up.res3.0.conv1.weight", "backbone.bottom_up.res3.0.conv1.norm.weight", "backbone.bottom_up.res3.0.conv1.norm.bias", "backbone.bottom_up.res3.0.conv1.norm.running_mean", "backbone.bottom_up.res3.0.conv1.norm.running_var", "backbone.bottom_up.res3.0.conv2.weight", "backbone.bottom_up.res3.0.conv2.norm.weight", "backbone.bottom_up.res3.0.conv2.norm.bias", "backbone.bottom_up.res3.0.conv2.norm.running_mean", "backbone.bottom_up.res3.0.conv2.norm.running_var", "backbone.bottom_up.res3.0.conv3.weight", "backbone.bottom_up.res3.0.conv3.norm.weight", "backbone.bottom_up.res3.0.conv3.norm.bias", "backbone.bottom_up.res3.0.conv3.norm.running_mean", "backbone.bottom_up.res3.0.conv3.norm.running_var", "backbone.bottom_up.res3.1.conv1.weight", "backbone.bottom_up.res3.1.conv1.norm.weight", "backbone.bottom_up.res3.1.conv1.norm.bias", "backbone.bottom_up.res3.1.conv1.norm.running_mean", "backbone.bottom_up.res3.1.conv1.norm.running_var", "backbone.bottom_up.res3.1.conv2.weight", "backbone.bottom_up.res3.1.conv2.norm.weight", "backbone.bottom_up.res3.1.conv2.norm.bias", "backbone.bottom_up.res3.1.conv2.norm.running_mean", "backbone.bottom_up.res3.1.conv2.norm.running_var", "backbone.bottom_up.res3.1.conv3.weight", "backbone.bottom_up.res3.1.conv3.norm.weight", "backbone.bottom_up.res3.1.conv3.norm.bias", "backbone.bottom_up.res3.1.conv3.norm.running_mean", "backbone.bottom_up.res3.1.conv3.norm.running_var", "backbone.bottom_up.res3.2.conv1.weight", "backbone.bottom_up.res3.2.conv1.norm.weight", "backbone.bottom_up.res3.2.conv1.norm.bias", "backbone.bottom_up.res3.2.conv1.norm.running_mean", "backbone.bottom_up.res3.2.conv1.norm.running_var", "backbone.bottom_up.res3.2.conv2.weight", "backbone.bottom_up.res3.2.conv2.norm.weight", "backbone.bottom_up.res3.2.conv2.norm.bias", "backbone.bottom_up.res3.2.conv2.norm.running_mean", "backbone.bottom_up.res3.2.conv2.norm.running_var", "backbone.bottom_up.res3.2.conv3.weight", "backbone.bottom_up.res3.2.conv3.norm.weight", "backbone.bottom_up.res3.2.conv3.norm.bias", "backbone.bottom_up.res3.2.conv3.norm.running_mean", "backbone.bottom_up.res3.2.conv3.norm.running_var", "backbone.bottom_up.res3.3.conv1.weight", "backbone.bottom_up.res3.3.conv1.norm.weight", "backbone.bottom_up.res3.3.conv1.norm.bias", "backbone.bottom_up.res3.3.conv1.norm.running_mean", "backbone.bottom_up.res3.3.conv1.norm.running_var", "backbone.bottom_up.res3.3.conv2.weight", "backbone.bottom_up.res3.3.conv2.norm.weight", "backbone.bottom_up.res3.3.conv2.norm.bias", "backbone.bottom_up.res3.3.conv2.norm.running_mean", "backbone.bottom_up.res3.3.conv2.norm.running_var", "backbone.bottom_up.res3.3.conv3.weight", "backbone.bottom_up.res3.3.conv3.norm.weight", "backbone.bottom_up.res3.3.conv3.norm.bias", "backbone.bottom_up.res3.3.conv3.norm.running_mean", "backbone.bottom_up.res3.3.conv3.norm.running_var", "backbone.bottom_up.res4.0.shortcut.weight", "backbone.bottom_up.res4.0.shortcut.norm.weight", "backbone.bottom_up.res4.0.shortcut.norm.bias", "backbone.bottom_up.res4.0.shortcut.norm.running_mean", "backbone.bottom_up.res4.0.shortcut.norm.running_var", "backbone.bottom_up.res4.0.conv1.weight", "backbone.bottom_up.res4.0.conv1.norm.weight", "backbone.bottom_up.res4.0.conv1.norm.bias", "backbone.bottom_up.res4.0.conv1.norm.running_mean", "backbone.bottom_up.res4.0.conv1.norm.running_var", "backbone.bottom_up.res4.0.conv2.weight", "backbone.bottom_up.res4.0.conv2.norm.weight", "backbone.bottom_up.res4.0.conv2.norm.bias", "backbone.bottom_up.res4.0.conv2.norm.running_mean", "backbone.bottom_up.res4.0.conv2.norm.running_var", "backbone.bottom_up.res4.0.conv3.weight", "backbone.bottom_up.res4.0.conv3.norm.weight", "backbone.bottom_up.res4.0.conv3.norm.bias", "backbone.bottom_up.res4.0.conv3.norm.running_mean", "backbone.bottom_up.res4.0.conv3.norm.running_var", "backbone.bottom_up.res4.1.conv1.weight", "backbone.bottom_up.res4.1.conv1.norm.weight", "backbone.bottom_up.res4.1.conv1.norm.bias", "backbone.bottom_up.res4.1.conv1.norm.running_mean", "backbone.bottom_up.res4.1.conv1.norm.running_var", "backbone.bottom_up.res4.1.conv2.weight", "backbone.bottom_up.res4.1.conv2.norm.weight", "backbone.bottom_up.res4.1.conv2.norm.bias", "backbone.bottom_up.res4.1.conv2.norm.running_mean", "backbone.bottom_up.res4.1.conv2.norm.running_var", "backbone.bottom_up.res4.1.conv3.weight", "backbone.bottom_up.res4.1.conv3.norm.weight", "backbone.bottom_up.res4.1.conv3.norm.bias", "backbone.bottom_up.res4.1.conv3.norm.running_mean", "backbone.bottom_up.res4.1.conv3.norm.running_var", "backbone.bottom_up.res4.2.conv1.weight", "backbone.bottom_up.res4.2.conv1.norm.weight", "backbone.bottom_up.res4.2.conv1.norm.bias", "backbone.bottom_up.res4.2.conv1.norm.running_mean", "backbone.bottom_up.res4.2.conv1.norm.running_var", "backbone.bottom_up.res4.2.conv2.weight", "backbone.bottom_up.res4.2.conv2.norm.weight", "backbone.bottom_up.res4.2.conv2.norm.bias", "backbone.bottom_up.res4.2.conv2.norm.running_mean", "backbone.bottom_up.res4.2.conv2.norm.running_var", "backbone.bottom_up.res4.2.conv3.weight", "backbone.bottom_up.res4.2.conv3.norm.weight", "backbone.bottom_up.res4.2.conv3.norm.bias", "backbone.bottom_up.res4.2.conv3.norm.running_mean", "backbone.bottom_up.res4.2.conv3.norm.running_var", "backbone.bottom_up.res4.3.conv1.weight", "backbone.bottom_up.res4.3.conv1.norm.weight", "backbone.bottom_up.res4.3.conv1.norm.bias", "backbone.bottom_up.res4.3.conv1.norm.running_mean", "backbone.bottom_up.res4.3.conv1.norm.running_var", "backbone.bottom_up.res4.3.conv2.weight", "backbone.bottom_up.res4.3.conv2.norm.weight", "backbone.bottom_up.res4.3.conv2.norm.bias", "backbone.bottom_up.res4.3.conv2.norm.running_mean", "backbone.bottom_up.res4.3.conv2.norm.running_var", "backbone.bottom_up.res4.3.conv3.weight", "backbone.bottom_up.res4.3.conv3.norm.weight", "backbone.bottom_up.res4.3.conv3.norm.bias", "backbone.bottom_up.res4.3.conv3.norm.running_mean", "backbone.bottom_up.res4.3.conv3.norm.running_var", "backbone.bottom_up.res4.4.conv1.weight", "backbone.bottom_up.res4.4.conv1.norm.weight", "backbone.bottom_up.res4.4.conv1.norm.bias", "backbone.bottom_up.res4.4.conv1.norm.running_mean", "backbone.bottom_up.res4.4.conv1.norm.running_var", "backbone.bottom_up.res4.4.conv2.weight", "backbone.bottom_up.res4.4.conv2.norm.weight", "backbone.bottom_up.res4.4.conv2.norm.bias", "backbone.bottom_up.res4.4.conv2.norm.running_mean", "backbone.bottom_up.res4.4.conv2.norm.running_var", "backbone.bottom_up.res4.4.conv3.weight", "backbone.bottom_up.res4.4.conv3.norm.weight", "backbone.bottom_up.res4.4.conv3.norm.bias", "backbone.bottom_up.res4.4.conv3.norm.running_mean", "backbone.bottom_up.res4.4.conv3.norm.running_var", "backbone.bottom_up.res4.5.conv1.weight", "backbone.bottom_up.res4.5.conv1.norm.weight", "backbone.bottom_up.res4.5.conv1.norm.bias", "backbone.bottom_up.res4.5.conv1.norm.running_mean", "backbone.bottom_up.res4.5.conv1.norm.running_var", "backbone.bottom_up.res4.5.conv2.weight", "backbone.bottom_up.res4.5.conv2.norm.weight", "backbone.bottom_up.res4.5.conv2.norm.bias", "backbone.bottom_up.res4.5.conv2.norm.running_mean", "backbone.bottom_up.res4.5.conv2.norm.running_var", "backbone.bottom_up.res4.5.conv3.weight", "backbone.bottom_up.res4.5.conv3.norm.weight", "backbone.bottom_up.res4.5.conv3.norm.bias", "backbone.bottom_up.res4.5.conv3.norm.running_mean", "backbone.bottom_up.res4.5.conv3.norm.running_var", "backbone.bottom_up.res5.0.shortcut.weight", "backbone.bottom_up.res5.0.shortcut.norm.weight", "backbone.bottom_up.res5.0.shortcut.norm.bias", "backbone.bottom_up.res5.0.shortcut.norm.running_mean", "backbone.bottom_up.res5.0.shortcut.norm.running_var", "backbone.bottom_up.res5.0.conv1.weight", "backbone.bottom_up.res5.0.conv1.norm.weight", "backbone.bottom_up.res5.0.conv1.norm.bias", "backbone.bottom_up.res5.0.conv1.norm.running_mean", "backbone.bottom_up.res5.0.conv1.norm.running_var", "backbone.bottom_up.res5.0.conv2.weight", "backbone.bottom_up.res5.0.conv2.norm.weight", "backbone.bottom_up.res5.0.conv2.norm.bias", "backbone.bottom_up.res5.0.conv2.norm.running_mean", "backbone.bottom_up.res5.0.conv2.norm.running_var", "backbone.bottom_up.res5.0.conv3.weight", "backbone.bottom_up.res5.0.conv3.norm.weight", "backbone.bottom_up.res5.0.conv3.norm.bias", "backbone.bottom_up.res5.0.conv3.norm.running_mean", "backbone.bottom_up.res5.0.conv3.norm.running_var", "backbone.bottom_up.res5.1.conv1.weight", "backbone.bottom_up.res5.1.conv1.norm.weight", "backbone.bottom_up.res5.1.conv1.norm.bias", "backbone.bottom_up.res5.1.conv1.norm.running_mean", "backbone.bottom_up.res5.1.conv1.norm.running_var", "backbone.bottom_up.res5.1.conv2.weight", "backbone.bottom_up.res5.1.conv2.norm.weight", "backbone.bottom_up.res5.1.conv2.norm.bias", "backbone.bottom_up.res5.1.conv2.norm.running_mean", "backbone.bottom_up.res5.1.conv2.norm.running_var", "backbone.bottom_up.res5.1.conv3.weight", "backbone.bottom_up.res5.1.conv3.norm.weight", "backbone.bottom_up.res5.1.conv3.norm.bias", "backbone.bottom_up.res5.1.conv3.norm.running_mean", "backbone.bottom_up.res5.1.conv3.norm.running_var", "backbone.bottom_up.res5.2.conv1.weight", "backbone.bottom_up.res5.2.conv1.norm.weight", "backbone.bottom_up.res5.2.conv1.norm.bias", "backbone.bottom_up.res5.2.conv1.norm.running_mean", "backbone.bottom_up.res5.2.conv1.norm.running_var", "backbone.bottom_up.res5.2.conv2.weight", "backbone.bottom_up.res5.2.conv2.norm.weight", "backbone.bottom_up.res5.2.conv2.norm.bias", "backbone.bottom_up.res5.2.conv2.norm.running_mean", "backbone.bottom_up.res5.2.conv2.norm.running_var", "backbone.bottom_up.res5.2.conv3.weight", "backbone.bottom_up.res5.2.conv3.norm.weight", "backbone.bottom_up.res5.2.conv3.norm.bias", "backbone.bottom_up.res5.2.conv3.norm.running_mean", "backbone.bottom_up.res5.2.conv3.norm.running_var", "anchor_generator.cell_anchors.0", "anchor_generator.cell_anchors.1", "anchor_generator.cell_anchors.2", "anchor_generator.cell_anchors.3", "anchor_generator.cell_anchors.4", "head.cls_subnet.0.weight", "head.cls_subnet.0.bias", "head.cls_subnet.2.weight", "head.cls_subnet.2.bias", "head.cls_subnet.4.weight", "head.cls_subnet.4.bias", "head.cls_subnet.6.weight", "head.cls_subnet.6.bias", "head.bbox_subnet.0.weight", "head.bbox_subnet.0.bias", "head.bbox_subnet.2.weight", "head.bbox_subnet.2.bias", "head.bbox_subnet.4.weight", "head.bbox_subnet.4.bias", "head.bbox_subnet.6.weight", "head.bbox_subnet.6.bias", "head.cls_score.weight", "head.cls_score.bias", "head.bbox_pred.weight", "head.bbox_pred.bias". 
2021-03-11 18:55:37,287 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:55:37,287 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:55:37,290 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:55:37,292 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:55:37,293 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:55:37,294 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:55:37,295 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:55:37,296 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:55:37,299 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:55:37,299 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:55:38,491 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:55:38,492 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]9872
2021-03-11 18:55:38,492 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:55:38,492 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:55:38,495 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:55:38,495 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:55:38,498 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:55:40,633 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:55:40,634 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:55:40,635 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:55:40,636 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:55:40,637 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:55:40,638 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:55:40,638 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:55:40,639 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:55:40,639 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:55:40,640 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:55:40,641 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:55:40,641 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:55:40,642 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:55:40,643 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:55:40,643 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:55:40,644 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:55:40,647 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:55:40,649 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:55:40,650 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:55:40,651 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:55:40,653 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:55:40,653 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:55:40,654 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:55:40,657 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:55:40,664 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "backbone.fpn_lateral3.weight", "backbone.fpn_lateral3.bias", "backbone.fpn_output3.weight", "backbone.fpn_output3.bias", "backbone.fpn_lateral4.weight", "backbone.fpn_lateral4.bias", "backbone.fpn_output4.weight", "backbone.fpn_output4.bias", "backbone.fpn_lateral5.weight", "backbone.fpn_lateral5.bias", "backbone.fpn_output5.weight", "backbone.fpn_output5.bias", "backbone.top_block.p6.weight", "backbone.top_block.p6.bias", "backbone.top_block.p7.weight", "backbone.top_block.p7.bias", "backbone.bottom_up.stem.conv1.weight", "backbone.bottom_up.stem.conv1.norm.weight", "backbone.bottom_up.stem.conv1.norm.bias", "backbone.bottom_up.stem.conv1.norm.running_mean", "backbone.bottom_up.stem.conv1.norm.running_var", "backbone.bottom_up.res2.0.shortcut.weight", "backbone.bottom_up.res2.0.shortcut.norm.weight", "backbone.bottom_up.res2.0.shortcut.norm.bias", "backbone.bottom_up.res2.0.shortcut.norm.running_mean", "backbone.bottom_up.res2.0.shortcut.norm.running_var", "backbone.bottom_up.res2.0.conv1.weight", "backbone.bottom_up.res2.0.conv1.norm.weight", "backbone.bottom_up.res2.0.conv1.norm.bias", "backbone.bottom_up.res2.0.conv1.norm.running_mean", "backbone.bottom_up.res2.0.conv1.norm.running_var", "backbone.bottom_up.res2.0.conv2.weight", "backbone.bottom_up.res2.0.conv2.norm.weight", "backbone.bottom_up.res2.0.conv2.norm.bias", "backbone.bottom_up.res2.0.conv2.norm.running_mean", "backbone.bottom_up.res2.0.conv2.norm.running_var", "backbone.bottom_up.res2.0.conv3.weight", "backbone.bottom_up.res2.0.conv3.norm.weight", "backbone.bottom_up.res2.0.conv3.norm.bias", "backbone.bottom_up.res2.0.conv3.norm.running_mean", "backbone.bottom_up.res2.0.conv3.norm.running_var", "backbone.bottom_up.res2.1.conv1.weight", "backbone.bottom_up.res2.1.conv1.norm.weight", "backbone.bottom_up.res2.1.conv1.norm.bias", "backbone.bottom_up.res2.1.conv1.norm.running_mean", "backbone.bottom_up.res2.1.conv1.norm.running_var", "backbone.bottom_up.res2.1.conv2.weight", "backbone.bottom_up.res2.1.conv2.norm.weight", "backbone.bottom_up.res2.1.conv2.norm.bias", "backbone.bottom_up.res2.1.conv2.norm.running_mean", "backbone.bottom_up.res2.1.conv2.norm.running_var", "backbone.bottom_up.res2.1.conv3.weight", "backbone.bottom_up.res2.1.conv3.norm.weight", "backbone.bottom_up.res2.1.conv3.norm.bias", "backbone.bottom_up.res2.1.conv3.norm.running_mean", "backbone.bottom_up.res2.1.conv3.norm.running_var", "backbone.bottom_up.res2.2.conv1.weight", "backbone.bottom_up.res2.2.conv1.norm.weight", "backbone.bottom_up.res2.2.conv1.norm.bias", "backbone.bottom_up.res2.2.conv1.norm.running_mean", "backbone.bottom_up.res2.2.conv1.norm.running_var", "backbone.bottom_up.res2.2.conv2.weight", "backbone.bottom_up.res2.2.conv2.norm.weight", "backbone.bottom_up.res2.2.conv2.norm.bias", "backbone.bottom_up.res2.2.conv2.norm.running_mean", "backbone.bottom_up.res2.2.conv2.norm.running_var", "backbone.bottom_up.res2.2.conv3.weight", "backbone.bottom_up.res2.2.conv3.norm.weight", "backbone.bottom_up.res2.2.conv3.norm.bias", "backbone.bottom_up.res2.2.conv3.norm.running_mean", "backbone.bottom_up.res2.2.conv3.norm.running_var", "backbone.bottom_up.res3.0.shortcut.weight", "backbone.bottom_up.res3.0.shortcut.norm.weight", "backbone.bottom_up.res3.0.shortcut.norm.bias", "backbone.bottom_up.res3.0.shortcut.norm.running_mean", "backbone.bottom_up.res3.0.shortcut.norm.running_var", "backbone.bottom_up.res3.0.conv1.weight", "backbone.bottom_up.res3.0.conv1.norm.weight", "backbone.bottom_up.res3.0.conv1.norm.bias", "backbone.bottom_up.res3.0.conv1.norm.running_mean", "backbone.bottom_up.res3.0.conv1.norm.running_var", "backbone.bottom_up.res3.0.conv2.weight", "backbone.bottom_up.res3.0.conv2.norm.weight", "backbone.bottom_up.res3.0.conv2.norm.bias", "backbone.bottom_up.res3.0.conv2.norm.running_mean", "backbone.bottom_up.res3.0.conv2.norm.running_var", "backbone.bottom_up.res3.0.conv3.weight", "backbone.bottom_up.res3.0.conv3.norm.weight", "backbone.bottom_up.res3.0.conv3.norm.bias", "backbone.bottom_up.res3.0.conv3.norm.running_mean", "backbone.bottom_up.res3.0.conv3.norm.running_var", "backbone.bottom_up.res3.1.conv1.weight", "backbone.bottom_up.res3.1.conv1.norm.weight", "backbone.bottom_up.res3.1.conv1.norm.bias", "backbone.bottom_up.res3.1.conv1.norm.running_mean", "backbone.bottom_up.res3.1.conv1.norm.running_var", "backbone.bottom_up.res3.1.conv2.weight", "backbone.bottom_up.res3.1.conv2.norm.weight", "backbone.bottom_up.res3.1.conv2.norm.bias", "backbone.bottom_up.res3.1.conv2.norm.running_mean", "backbone.bottom_up.res3.1.conv2.norm.running_var", "backbone.bottom_up.res3.1.conv3.weight", "backbone.bottom_up.res3.1.conv3.norm.weight", "backbone.bottom_up.res3.1.conv3.norm.bias", "backbone.bottom_up.res3.1.conv3.norm.running_mean", "backbone.bottom_up.res3.1.conv3.norm.running_var", "backbone.bottom_up.res3.2.conv1.weight", "backbone.bottom_up.res3.2.conv1.norm.weight", "backbone.bottom_up.res3.2.conv1.norm.bias", "backbone.bottom_up.res3.2.conv1.norm.running_mean", "backbone.bottom_up.res3.2.conv1.norm.running_var", "backbone.bottom_up.res3.2.conv2.weight", "backbone.bottom_up.res3.2.conv2.norm.weight", "backbone.bottom_up.res3.2.conv2.norm.bias", "backbone.bottom_up.res3.2.conv2.norm.running_mean", "backbone.bottom_up.res3.2.conv2.norm.running_var", "backbone.bottom_up.res3.2.conv3.weight", "backbone.bottom_up.res3.2.conv3.norm.weight", "backbone.bottom_up.res3.2.conv3.norm.bias", "backbone.bottom_up.res3.2.conv3.norm.running_mean", "backbone.bottom_up.res3.2.conv3.norm.running_var", "backbone.bottom_up.res3.3.conv1.weight", "backbone.bottom_up.res3.3.conv1.norm.weight", "backbone.bottom_up.res3.3.conv1.norm.bias", "backbone.bottom_up.res3.3.conv1.norm.running_mean", "backbone.bottom_up.res3.3.conv1.norm.running_var", "backbone.bottom_up.res3.3.conv2.weight", "backbone.bottom_up.res3.3.conv2.norm.weight", "backbone.bottom_up.res3.3.conv2.norm.bias", "backbone.bottom_up.res3.3.conv2.norm.running_mean", "backbone.bottom_up.res3.3.conv2.norm.running_var", "backbone.bottom_up.res3.3.conv3.weight", "backbone.bottom_up.res3.3.conv3.norm.weight", "backbone.bottom_up.res3.3.conv3.norm.bias", "backbone.bottom_up.res3.3.conv3.norm.running_mean", "backbone.bottom_up.res3.3.conv3.norm.running_var", "backbone.bottom_up.res4.0.shortcut.weight", "backbone.bottom_up.res4.0.shortcut.norm.weight", "backbone.bottom_up.res4.0.shortcut.norm.bias", "backbone.bottom_up.res4.0.shortcut.norm.running_mean", "backbone.bottom_up.res4.0.shortcut.norm.running_var", "backbone.bottom_up.res4.0.conv1.weight", "backbone.bottom_up.res4.0.conv1.norm.weight", "backbone.bottom_up.res4.0.conv1.norm.bias", "backbone.bottom_up.res4.0.conv1.norm.running_mean", "backbone.bottom_up.res4.0.conv1.norm.running_var", "backbone.bottom_up.res4.0.conv2.weight", "backbone.bottom_up.res4.0.conv2.norm.weight", "backbone.bottom_up.res4.0.conv2.norm.bias", "backbone.bottom_up.res4.0.conv2.norm.running_mean", "backbone.bottom_up.res4.0.conv2.norm.running_var", "backbone.bottom_up.res4.0.conv3.weight", "backbone.bottom_up.res4.0.conv3.norm.weight", "backbone.bottom_up.res4.0.conv3.norm.bias", "backbone.bottom_up.res4.0.conv3.norm.running_mean", "backbone.bottom_up.res4.0.conv3.norm.running_var", "backbone.bottom_up.res4.1.conv1.weight", "backbone.bottom_up.res4.1.conv1.norm.weight", "backbone.bottom_up.res4.1.conv1.norm.bias", "backbone.bottom_up.res4.1.conv1.norm.running_mean", "backbone.bottom_up.res4.1.conv1.norm.running_var", "backbone.bottom_up.res4.1.conv2.weight", "backbone.bottom_up.res4.1.conv2.norm.weight", "backbone.bottom_up.res4.1.conv2.norm.bias", "backbone.bottom_up.res4.1.conv2.norm.running_mean", "backbone.bottom_up.res4.1.conv2.norm.running_var", "backbone.bottom_up.res4.1.conv3.weight", "backbone.bottom_up.res4.1.conv3.norm.weight", "backbone.bottom_up.res4.1.conv3.norm.bias", "backbone.bottom_up.res4.1.conv3.norm.running_mean", "backbone.bottom_up.res4.1.conv3.norm.running_var", "backbone.bottom_up.res4.2.conv1.weight", "backbone.bottom_up.res4.2.conv1.norm.weight", "backbone.bottom_up.res4.2.conv1.norm.bias", "backbone.bottom_up.res4.2.conv1.norm.running_mean", "backbone.bottom_up.res4.2.conv1.norm.running_var", "backbone.bottom_up.res4.2.conv2.weight", "backbone.bottom_up.res4.2.conv2.norm.weight", "backbone.bottom_up.res4.2.conv2.norm.bias", "backbone.bottom_up.res4.2.conv2.norm.running_mean", "backbone.bottom_up.res4.2.conv2.norm.running_var", "backbone.bottom_up.res4.2.conv3.weight", "backbone.bottom_up.res4.2.conv3.norm.weight", "backbone.bottom_up.res4.2.conv3.norm.bias", "backbone.bottom_up.res4.2.conv3.norm.running_mean", "backbone.bottom_up.res4.2.conv3.norm.running_var", "backbone.bottom_up.res4.3.conv1.weight", "backbone.bottom_up.res4.3.conv1.norm.weight", "backbone.bottom_up.res4.3.conv1.norm.bias", "backbone.bottom_up.res4.3.conv1.norm.running_mean", "backbone.bottom_up.res4.3.conv1.norm.running_var", "backbone.bottom_up.res4.3.conv2.weight", "backbone.bottom_up.res4.3.conv2.norm.weight", "backbone.bottom_up.res4.3.conv2.norm.bias", "backbone.bottom_up.res4.3.conv2.norm.running_mean", "backbone.bottom_up.res4.3.conv2.norm.running_var", "backbone.bottom_up.res4.3.conv3.weight", "backbone.bottom_up.res4.3.conv3.norm.weight", "backbone.bottom_up.res4.3.conv3.norm.bias", "backbone.bottom_up.res4.3.conv3.norm.running_mean", "backbone.bottom_up.res4.3.conv3.norm.running_var", "backbone.bottom_up.res4.4.conv1.weight", "backbone.bottom_up.res4.4.conv1.norm.weight", "backbone.bottom_up.res4.4.conv1.norm.bias", "backbone.bottom_up.res4.4.conv1.norm.running_mean", "backbone.bottom_up.res4.4.conv1.norm.running_var", "backbone.bottom_up.res4.4.conv2.weight", "backbone.bottom_up.res4.4.conv2.norm.weight", "backbone.bottom_up.res4.4.conv2.norm.bias", "backbone.bottom_up.res4.4.conv2.norm.running_mean", "backbone.bottom_up.res4.4.conv2.norm.running_var", "backbone.bottom_up.res4.4.conv3.weight", "backbone.bottom_up.res4.4.conv3.norm.weight", "backbone.bottom_up.res4.4.conv3.norm.bias", "backbone.bottom_up.res4.4.conv3.norm.running_mean", "backbone.bottom_up.res4.4.conv3.norm.running_var", "backbone.bottom_up.res4.5.conv1.weight", "backbone.bottom_up.res4.5.conv1.norm.weight", "backbone.bottom_up.res4.5.conv1.norm.bias", "backbone.bottom_up.res4.5.conv1.norm.running_mean", "backbone.bottom_up.res4.5.conv1.norm.running_var", "backbone.bottom_up.res4.5.conv2.weight", "backbone.bottom_up.res4.5.conv2.norm.weight", "backbone.bottom_up.res4.5.conv2.norm.bias", "backbone.bottom_up.res4.5.conv2.norm.running_mean", "backbone.bottom_up.res4.5.conv2.norm.running_var", "backbone.bottom_up.res4.5.conv3.weight", "backbone.bottom_up.res4.5.conv3.norm.weight", "backbone.bottom_up.res4.5.conv3.norm.bias", "backbone.bottom_up.res4.5.conv3.norm.running_mean", "backbone.bottom_up.res4.5.conv3.norm.running_var", "backbone.bottom_up.res5.0.shortcut.weight", "backbone.bottom_up.res5.0.shortcut.norm.weight", "backbone.bottom_up.res5.0.shortcut.norm.bias", "backbone.bottom_up.res5.0.shortcut.norm.running_mean", "backbone.bottom_up.res5.0.shortcut.norm.running_var", "backbone.bottom_up.res5.0.conv1.weight", "backbone.bottom_up.res5.0.conv1.norm.weight", "backbone.bottom_up.res5.0.conv1.norm.bias", "backbone.bottom_up.res5.0.conv1.norm.running_mean", "backbone.bottom_up.res5.0.conv1.norm.running_var", "backbone.bottom_up.res5.0.conv2.weight", "backbone.bottom_up.res5.0.conv2.norm.weight", "backbone.bottom_up.res5.0.conv2.norm.bias", "backbone.bottom_up.res5.0.conv2.norm.running_mean", "backbone.bottom_up.res5.0.conv2.norm.running_var", "backbone.bottom_up.res5.0.conv3.weight", "backbone.bottom_up.res5.0.conv3.norm.weight", "backbone.bottom_up.res5.0.conv3.norm.bias", "backbone.bottom_up.res5.0.conv3.norm.running_mean", "backbone.bottom_up.res5.0.conv3.norm.running_var", "backbone.bottom_up.res5.1.conv1.weight", "backbone.bottom_up.res5.1.conv1.norm.weight", "backbone.bottom_up.res5.1.conv1.norm.bias", "backbone.bottom_up.res5.1.conv1.norm.running_mean", "backbone.bottom_up.res5.1.conv1.norm.running_var", "backbone.bottom_up.res5.1.conv2.weight", "backbone.bottom_up.res5.1.conv2.norm.weight", "backbone.bottom_up.res5.1.conv2.norm.bias", "backbone.bottom_up.res5.1.conv2.norm.running_mean", "backbone.bottom_up.res5.1.conv2.norm.running_var", "backbone.bottom_up.res5.1.conv3.weight", "backbone.bottom_up.res5.1.conv3.norm.weight", "backbone.bottom_up.res5.1.conv3.norm.bias", "backbone.bottom_up.res5.1.conv3.norm.running_mean", "backbone.bottom_up.res5.1.conv3.norm.running_var", "backbone.bottom_up.res5.2.conv1.weight", "backbone.bottom_up.res5.2.conv1.norm.weight", "backbone.bottom_up.res5.2.conv1.norm.bias", "backbone.bottom_up.res5.2.conv1.norm.running_mean", "backbone.bottom_up.res5.2.conv1.norm.running_var", "backbone.bottom_up.res5.2.conv2.weight", "backbone.bottom_up.res5.2.conv2.norm.weight", "backbone.bottom_up.res5.2.conv2.norm.bias", "backbone.bottom_up.res5.2.conv2.norm.running_mean", "backbone.bottom_up.res5.2.conv2.norm.running_var", "backbone.bottom_up.res5.2.conv3.weight", "backbone.bottom_up.res5.2.conv3.norm.weight", "backbone.bottom_up.res5.2.conv3.norm.bias", "backbone.bottom_up.res5.2.conv3.norm.running_mean", "backbone.bottom_up.res5.2.conv3.norm.running_var", "anchor_generator.cell_anchors.0", "anchor_generator.cell_anchors.1", "anchor_generator.cell_anchors.2", "anchor_generator.cell_anchors.3", "anchor_generator.cell_anchors.4", "head.cls_subnet.0.weight", "head.cls_subnet.0.bias", "head.cls_subnet.2.weight", "head.cls_subnet.2.bias", "head.cls_subnet.4.weight", "head.cls_subnet.4.bias", "head.cls_subnet.6.weight", "head.cls_subnet.6.bias", "head.bbox_subnet.0.weight", "head.bbox_subnet.0.bias", "head.bbox_subnet.2.weight", "head.bbox_subnet.2.bias", "head.bbox_subnet.4.weight", "head.bbox_subnet.4.bias", "head.bbox_subnet.6.weight", "head.bbox_subnet.6.bias", "head.cls_score.weight", "head.cls_score.bias", "head.bbox_pred.weight", "head.bbox_pred.bias". 
2021-03-11 18:55:40,679 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:55:40,680 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:55:40,681 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:55:40,683 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:55:40,684 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:55:40,685 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:55:40,686 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:55:40,686 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-11 18:55:40,687 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:55:40,687 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:55:41,878 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:55:42,459 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]3068
2021-03-11 18:55:42,462 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:55:42,462 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:55:42,462 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:55:42,465 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:55:42,469 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:55:44,613 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-11 18:55:44,614 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-11 18:55:44,615 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-11 18:55:44,617 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-11 18:55:44,617 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-11 18:55:44,618 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-11 18:55:44,618 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-11 18:55:44,619 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-11 18:55:44,620 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-11 18:55:44,620 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-11 18:55:44,621 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-11 18:55:44,621 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-11 18:55:44,623 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-11 18:55:44,623 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:55:44,625 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-11 18:55:44,626 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-11 18:55:44,626 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-11 18:55:44,627 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-11 18:55:44,627 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-11 18:55:44,628 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-11 18:55:44,629 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-11 18:55:44,629 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-11 18:55:44,632 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-11 18:55:44,638 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-11 18:55:44,649 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "backbone.fpn_lateral3.weight", "backbone.fpn_lateral3.bias", "backbone.fpn_output3.weight", "backbone.fpn_output3.bias", "backbone.fpn_lateral4.weight", "backbone.fpn_lateral4.bias", "backbone.fpn_output4.weight", "backbone.fpn_output4.bias", "backbone.fpn_lateral5.weight", "backbone.fpn_lateral5.bias", "backbone.fpn_output5.weight", "backbone.fpn_output5.bias", "backbone.top_block.p6.weight", "backbone.top_block.p6.bias", "backbone.top_block.p7.weight", "backbone.top_block.p7.bias", "backbone.bottom_up.stem.conv1.weight", "backbone.bottom_up.stem.conv1.norm.weight", "backbone.bottom_up.stem.conv1.norm.bias", "backbone.bottom_up.stem.conv1.norm.running_mean", "backbone.bottom_up.stem.conv1.norm.running_var", "backbone.bottom_up.res2.0.shortcut.weight", "backbone.bottom_up.res2.0.shortcut.norm.weight", "backbone.bottom_up.res2.0.shortcut.norm.bias", "backbone.bottom_up.res2.0.shortcut.norm.running_mean", "backbone.bottom_up.res2.0.shortcut.norm.running_var", "backbone.bottom_up.res2.0.conv1.weight", "backbone.bottom_up.res2.0.conv1.norm.weight", "backbone.bottom_up.res2.0.conv1.norm.bias", "backbone.bottom_up.res2.0.conv1.norm.running_mean", "backbone.bottom_up.res2.0.conv1.norm.running_var", "backbone.bottom_up.res2.0.conv2.weight", "backbone.bottom_up.res2.0.conv2.norm.weight", "backbone.bottom_up.res2.0.conv2.norm.bias", "backbone.bottom_up.res2.0.conv2.norm.running_mean", "backbone.bottom_up.res2.0.conv2.norm.running_var", "backbone.bottom_up.res2.0.conv3.weight", "backbone.bottom_up.res2.0.conv3.norm.weight", "backbone.bottom_up.res2.0.conv3.norm.bias", "backbone.bottom_up.res2.0.conv3.norm.running_mean", "backbone.bottom_up.res2.0.conv3.norm.running_var", "backbone.bottom_up.res2.1.conv1.weight", "backbone.bottom_up.res2.1.conv1.norm.weight", "backbone.bottom_up.res2.1.conv1.norm.bias", "backbone.bottom_up.res2.1.conv1.norm.running_mean", "backbone.bottom_up.res2.1.conv1.norm.running_var", "backbone.bottom_up.res2.1.conv2.weight", "backbone.bottom_up.res2.1.conv2.norm.weight", "backbone.bottom_up.res2.1.conv2.norm.bias", "backbone.bottom_up.res2.1.conv2.norm.running_mean", "backbone.bottom_up.res2.1.conv2.norm.running_var", "backbone.bottom_up.res2.1.conv3.weight", "backbone.bottom_up.res2.1.conv3.norm.weight", "backbone.bottom_up.res2.1.conv3.norm.bias", "backbone.bottom_up.res2.1.conv3.norm.running_mean", "backbone.bottom_up.res2.1.conv3.norm.running_var", "backbone.bottom_up.res2.2.conv1.weight", "backbone.bottom_up.res2.2.conv1.norm.weight", "backbone.bottom_up.res2.2.conv1.norm.bias", "backbone.bottom_up.res2.2.conv1.norm.running_mean", "backbone.bottom_up.res2.2.conv1.norm.running_var", "backbone.bottom_up.res2.2.conv2.weight", "backbone.bottom_up.res2.2.conv2.norm.weight", "backbone.bottom_up.res2.2.conv2.norm.bias", "backbone.bottom_up.res2.2.conv2.norm.running_mean", "backbone.bottom_up.res2.2.conv2.norm.running_var", "backbone.bottom_up.res2.2.conv3.weight", "backbone.bottom_up.res2.2.conv3.norm.weight", "backbone.bottom_up.res2.2.conv3.norm.bias", "backbone.bottom_up.res2.2.conv3.norm.running_mean", "backbone.bottom_up.res2.2.conv3.norm.running_var", "backbone.bottom_up.res3.0.shortcut.weight", "backbone.bottom_up.res3.0.shortcut.norm.weight", "backbone.bottom_up.res3.0.shortcut.norm.bias", "backbone.bottom_up.res3.0.shortcut.norm.running_mean", "backbone.bottom_up.res3.0.shortcut.norm.running_var", "backbone.bottom_up.res3.0.conv1.weight", "backbone.bottom_up.res3.0.conv1.norm.weight", "backbone.bottom_up.res3.0.conv1.norm.bias", "backbone.bottom_up.res3.0.conv1.norm.running_mean", "backbone.bottom_up.res3.0.conv1.norm.running_var", "backbone.bottom_up.res3.0.conv2.weight", "backbone.bottom_up.res3.0.conv2.norm.weight", "backbone.bottom_up.res3.0.conv2.norm.bias", "backbone.bottom_up.res3.0.conv2.norm.running_mean", "backbone.bottom_up.res3.0.conv2.norm.running_var", "backbone.bottom_up.res3.0.conv3.weight", "backbone.bottom_up.res3.0.conv3.norm.weight", "backbone.bottom_up.res3.0.conv3.norm.bias", "backbone.bottom_up.res3.0.conv3.norm.running_mean", "backbone.bottom_up.res3.0.conv3.norm.running_var", "backbone.bottom_up.res3.1.conv1.weight", "backbone.bottom_up.res3.1.conv1.norm.weight", "backbone.bottom_up.res3.1.conv1.norm.bias", "backbone.bottom_up.res3.1.conv1.norm.running_mean", "backbone.bottom_up.res3.1.conv1.norm.running_var", "backbone.bottom_up.res3.1.conv2.weight", "backbone.bottom_up.res3.1.conv2.norm.weight", "backbone.bottom_up.res3.1.conv2.norm.bias", "backbone.bottom_up.res3.1.conv2.norm.running_mean", "backbone.bottom_up.res3.1.conv2.norm.running_var", "backbone.bottom_up.res3.1.conv3.weight", "backbone.bottom_up.res3.1.conv3.norm.weight", "backbone.bottom_up.res3.1.conv3.norm.bias", "backbone.bottom_up.res3.1.conv3.norm.running_mean", "backbone.bottom_up.res3.1.conv3.norm.running_var", "backbone.bottom_up.res3.2.conv1.weight", "backbone.bottom_up.res3.2.conv1.norm.weight", "backbone.bottom_up.res3.2.conv1.norm.bias", "backbone.bottom_up.res3.2.conv1.norm.running_mean", "backbone.bottom_up.res3.2.conv1.norm.running_var", "backbone.bottom_up.res3.2.conv2.weight", "backbone.bottom_up.res3.2.conv2.norm.weight", "backbone.bottom_up.res3.2.conv2.norm.bias", "backbone.bottom_up.res3.2.conv2.norm.running_mean", "backbone.bottom_up.res3.2.conv2.norm.running_var", "backbone.bottom_up.res3.2.conv3.weight", "backbone.bottom_up.res3.2.conv3.norm.weight", "backbone.bottom_up.res3.2.conv3.norm.bias", "backbone.bottom_up.res3.2.conv3.norm.running_mean", "backbone.bottom_up.res3.2.conv3.norm.running_var", "backbone.bottom_up.res3.3.conv1.weight", "backbone.bottom_up.res3.3.conv1.norm.weight", "backbone.bottom_up.res3.3.conv1.norm.bias", "backbone.bottom_up.res3.3.conv1.norm.running_mean", "backbone.bottom_up.res3.3.conv1.norm.running_var", "backbone.bottom_up.res3.3.conv2.weight", "backbone.bottom_up.res3.3.conv2.norm.weight", "backbone.bottom_up.res3.3.conv2.norm.bias", "backbone.bottom_up.res3.3.conv2.norm.running_mean", "backbone.bottom_up.res3.3.conv2.norm.running_var", "backbone.bottom_up.res3.3.conv3.weight", "backbone.bottom_up.res3.3.conv3.norm.weight", "backbone.bottom_up.res3.3.conv3.norm.bias", "backbone.bottom_up.res3.3.conv3.norm.running_mean", "backbone.bottom_up.res3.3.conv3.norm.running_var", "backbone.bottom_up.res4.0.shortcut.weight", "backbone.bottom_up.res4.0.shortcut.norm.weight", "backbone.bottom_up.res4.0.shortcut.norm.bias", "backbone.bottom_up.res4.0.shortcut.norm.running_mean", "backbone.bottom_up.res4.0.shortcut.norm.running_var", "backbone.bottom_up.res4.0.conv1.weight", "backbone.bottom_up.res4.0.conv1.norm.weight", "backbone.bottom_up.res4.0.conv1.norm.bias", "backbone.bottom_up.res4.0.conv1.norm.running_mean", "backbone.bottom_up.res4.0.conv1.norm.running_var", "backbone.bottom_up.res4.0.conv2.weight", "backbone.bottom_up.res4.0.conv2.norm.weight", "backbone.bottom_up.res4.0.conv2.norm.bias", "backbone.bottom_up.res4.0.conv2.norm.running_mean", "backbone.bottom_up.res4.0.conv2.norm.running_var", "backbone.bottom_up.res4.0.conv3.weight", "backbone.bottom_up.res4.0.conv3.norm.weight", "backbone.bottom_up.res4.0.conv3.norm.bias", "backbone.bottom_up.res4.0.conv3.norm.running_mean", "backbone.bottom_up.res4.0.conv3.norm.running_var", "backbone.bottom_up.res4.1.conv1.weight", "backbone.bottom_up.res4.1.conv1.norm.weight", "backbone.bottom_up.res4.1.conv1.norm.bias", "backbone.bottom_up.res4.1.conv1.norm.running_mean", "backbone.bottom_up.res4.1.conv1.norm.running_var", "backbone.bottom_up.res4.1.conv2.weight", "backbone.bottom_up.res4.1.conv2.norm.weight", "backbone.bottom_up.res4.1.conv2.norm.bias", "backbone.bottom_up.res4.1.conv2.norm.running_mean", "backbone.bottom_up.res4.1.conv2.norm.running_var", "backbone.bottom_up.res4.1.conv3.weight", "backbone.bottom_up.res4.1.conv3.norm.weight", "backbone.bottom_up.res4.1.conv3.norm.bias", "backbone.bottom_up.res4.1.conv3.norm.running_mean", "backbone.bottom_up.res4.1.conv3.norm.running_var", "backbone.bottom_up.res4.2.conv1.weight", "backbone.bottom_up.res4.2.conv1.norm.weight", "backbone.bottom_up.res4.2.conv1.norm.bias", "backbone.bottom_up.res4.2.conv1.norm.running_mean", "backbone.bottom_up.res4.2.conv1.norm.running_var", "backbone.bottom_up.res4.2.conv2.weight", "backbone.bottom_up.res4.2.conv2.norm.weight", "backbone.bottom_up.res4.2.conv2.norm.bias", "backbone.bottom_up.res4.2.conv2.norm.running_mean", "backbone.bottom_up.res4.2.conv2.norm.running_var", "backbone.bottom_up.res4.2.conv3.weight", "backbone.bottom_up.res4.2.conv3.norm.weight", "backbone.bottom_up.res4.2.conv3.norm.bias", "backbone.bottom_up.res4.2.conv3.norm.running_mean", "backbone.bottom_up.res4.2.conv3.norm.running_var", "backbone.bottom_up.res4.3.conv1.weight", "backbone.bottom_up.res4.3.conv1.norm.weight", "backbone.bottom_up.res4.3.conv1.norm.bias", "backbone.bottom_up.res4.3.conv1.norm.running_mean", "backbone.bottom_up.res4.3.conv1.norm.running_var", "backbone.bottom_up.res4.3.conv2.weight", "backbone.bottom_up.res4.3.conv2.norm.weight", "backbone.bottom_up.res4.3.conv2.norm.bias", "backbone.bottom_up.res4.3.conv2.norm.running_mean", "backbone.bottom_up.res4.3.conv2.norm.running_var", "backbone.bottom_up.res4.3.conv3.weight", "backbone.bottom_up.res4.3.conv3.norm.weight", "backbone.bottom_up.res4.3.conv3.norm.bias", "backbone.bottom_up.res4.3.conv3.norm.running_mean", "backbone.bottom_up.res4.3.conv3.norm.running_var", "backbone.bottom_up.res4.4.conv1.weight", "backbone.bottom_up.res4.4.conv1.norm.weight", "backbone.bottom_up.res4.4.conv1.norm.bias", "backbone.bottom_up.res4.4.conv1.norm.running_mean", "backbone.bottom_up.res4.4.conv1.norm.running_var", "backbone.bottom_up.res4.4.conv2.weight", "backbone.bottom_up.res4.4.conv2.norm.weight", "backbone.bottom_up.res4.4.conv2.norm.bias", "backbone.bottom_up.res4.4.conv2.norm.running_mean", "backbone.bottom_up.res4.4.conv2.norm.running_var", "backbone.bottom_up.res4.4.conv3.weight", "backbone.bottom_up.res4.4.conv3.norm.weight", "backbone.bottom_up.res4.4.conv3.norm.bias", "backbone.bottom_up.res4.4.conv3.norm.running_mean", "backbone.bottom_up.res4.4.conv3.norm.running_var", "backbone.bottom_up.res4.5.conv1.weight", "backbone.bottom_up.res4.5.conv1.norm.weight", "backbone.bottom_up.res4.5.conv1.norm.bias", "backbone.bottom_up.res4.5.conv1.norm.running_mean", "backbone.bottom_up.res4.5.conv1.norm.running_var", "backbone.bottom_up.res4.5.conv2.weight", "backbone.bottom_up.res4.5.conv2.norm.weight", "backbone.bottom_up.res4.5.conv2.norm.bias", "backbone.bottom_up.res4.5.conv2.norm.running_mean", "backbone.bottom_up.res4.5.conv2.norm.running_var", "backbone.bottom_up.res4.5.conv3.weight", "backbone.bottom_up.res4.5.conv3.norm.weight", "backbone.bottom_up.res4.5.conv3.norm.bias", "backbone.bottom_up.res4.5.conv3.norm.running_mean", "backbone.bottom_up.res4.5.conv3.norm.running_var", "backbone.bottom_up.res5.0.shortcut.weight", "backbone.bottom_up.res5.0.shortcut.norm.weight", "backbone.bottom_up.res5.0.shortcut.norm.bias", "backbone.bottom_up.res5.0.shortcut.norm.running_mean", "backbone.bottom_up.res5.0.shortcut.norm.running_var", "backbone.bottom_up.res5.0.conv1.weight", "backbone.bottom_up.res5.0.conv1.norm.weight", "backbone.bottom_up.res5.0.conv1.norm.bias", "backbone.bottom_up.res5.0.conv1.norm.running_mean", "backbone.bottom_up.res5.0.conv1.norm.running_var", "backbone.bottom_up.res5.0.conv2.weight", "backbone.bottom_up.res5.0.conv2.norm.weight", "backbone.bottom_up.res5.0.conv2.norm.bias", "backbone.bottom_up.res5.0.conv2.norm.running_mean", "backbone.bottom_up.res5.0.conv2.norm.running_var", "backbone.bottom_up.res5.0.conv3.weight", "backbone.bottom_up.res5.0.conv3.norm.weight", "backbone.bottom_up.res5.0.conv3.norm.bias", "backbone.bottom_up.res5.0.conv3.norm.running_mean", "backbone.bottom_up.res5.0.conv3.norm.running_var", "backbone.bottom_up.res5.1.conv1.weight", "backbone.bottom_up.res5.1.conv1.norm.weight", "backbone.bottom_up.res5.1.conv1.norm.bias", "backbone.bottom_up.res5.1.conv1.norm.running_mean", "backbone.bottom_up.res5.1.conv1.norm.running_var", "backbone.bottom_up.res5.1.conv2.weight", "backbone.bottom_up.res5.1.conv2.norm.weight", "backbone.bottom_up.res5.1.conv2.norm.bias", "backbone.bottom_up.res5.1.conv2.norm.running_mean", "backbone.bottom_up.res5.1.conv2.norm.running_var", "backbone.bottom_up.res5.1.conv3.weight", "backbone.bottom_up.res5.1.conv3.norm.weight", "backbone.bottom_up.res5.1.conv3.norm.bias", "backbone.bottom_up.res5.1.conv3.norm.running_mean", "backbone.bottom_up.res5.1.conv3.norm.running_var", "backbone.bottom_up.res5.2.conv1.weight", "backbone.bottom_up.res5.2.conv1.norm.weight", "backbone.bottom_up.res5.2.conv1.norm.bias", "backbone.bottom_up.res5.2.conv1.norm.running_mean", "backbone.bottom_up.res5.2.conv1.norm.running_var", "backbone.bottom_up.res5.2.conv2.weight", "backbone.bottom_up.res5.2.conv2.norm.weight", "backbone.bottom_up.res5.2.conv2.norm.bias", "backbone.bottom_up.res5.2.conv2.norm.running_mean", "backbone.bottom_up.res5.2.conv2.norm.running_var", "backbone.bottom_up.res5.2.conv3.weight", "backbone.bottom_up.res5.2.conv3.norm.weight", "backbone.bottom_up.res5.2.conv3.norm.bias", "backbone.bottom_up.res5.2.conv3.norm.running_mean", "backbone.bottom_up.res5.2.conv3.norm.running_var", "anchor_generator.cell_anchors.0", "anchor_generator.cell_anchors.1", "anchor_generator.cell_anchors.2", "anchor_generator.cell_anchors.3", "anchor_generator.cell_anchors.4", "head.cls_subnet.0.weight", "head.cls_subnet.0.bias", "head.cls_subnet.2.weight", "head.cls_subnet.2.bias", "head.cls_subnet.4.weight", "head.cls_subnet.4.bias", "head.cls_subnet.6.weight", "head.cls_subnet.6.bias", "head.bbox_subnet.0.weight", "head.bbox_subnet.0.bias", "head.bbox_subnet.2.weight", "head.bbox_subnet.2.bias", "head.bbox_subnet.4.weight", "head.bbox_subnet.4.bias", "head.bbox_subnet.6.weight", "head.bbox_subnet.6.bias", "head.cls_score.weight", "head.cls_score.bias", "head.bbox_pred.weight", "head.bbox_pred.bias". 
2021-03-11 18:55:44,663 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-11 18:55:44,666 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-11 18:55:44,666 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-11 18:55:44,668 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:55:44,669 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-11 18:55:44,670 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:55:44,670 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:55:44,671 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-11 18:55:44,672 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:55:44,672 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:55:45,241 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-11 18:55:45,241 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-11 18:55:45,244 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-11 18:55:46,865 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-11 18:55:46,867 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]11428
2021-03-11 18:55:46,867 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-11 18:55:46,868 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-11 18:55:46,868 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-11 18:55:46,870 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-11 18:55:46,873 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-11 18:55:47,305 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-11 18:55:47,318 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retina version 1.0
2021-03-11 18:55:47,319 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retina version: 1.0
2021-03-11 18:55:47,320 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_SCALED_DOWN
2021-03-11 18:55:47,321 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:55:47,322 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2021-03-11 18:55:47,323 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:55:47,324 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2021-03-11 18:55:47,325 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2021-03-11 18:55:47,327 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-11 18:55:47,327 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2021-03-11 18:55:47,328 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-11 18:55:47,333 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-11 18:55:47,333 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-11 18:55:47,333 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-11 18:55:47,333 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2021-03-11 18:55:47,440 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retina unregistered.
2021-03-12 01:16:02,282 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: logs\config\20210311182350412-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retina=retina.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 01:16:02,288 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20210311182350412-shutdown.cfg",
  "modelCount": 1,
  "created": 1615454630412,
  "models": {
    "retina": {
      "1.0": {
        "defaultVersion": true,
        "marName": "retina.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2021-03-12 01:16:02,297 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20210311182350412-shutdown.cfg
2021-03-12 01:16:02,299 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20210311182350412-shutdown.cfg validated successfully
2021-03-12 01:16:06,839 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 85be9e0ea46e4badb098f54e26b96991
2021-03-12 01:16:06,857 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-12 01:16:06,857 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-12 01:16:06,859 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-12 01:16:06,859 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-12 01:16:06,860 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-12 01:16:06,882 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-12 01:16:07,086 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:16:07,087 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]4532
2021-03-12 01:16:07,088 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:16:07,088 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-12 01:16:07,089 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:16:07,094 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:16:07,348 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-12 01:16:07,348 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-12 01:16:07,352 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-12 01:16:07,352 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-12 01:16:07,353 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-12 01:16:07,354 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:16:10,889 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:16:10,890 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:16:10,890 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:16:10,891 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:16:10,892 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:16:10,893 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:16:10,893 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:16:10,894 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:16:10,897 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:16:10,897 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:16:10,898 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:16:10,898 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:16:10,899 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:10,899 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:16:10,900 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:10,901 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 01:16:10,901 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:16:10,902 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 01:16:10,906 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:16:12,077 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:16:12,078 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]7240
2021-03-12 01:16:12,079 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:16:12,079 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:16:12,080 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:16:12,080 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:16:12,084 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:16:13,626 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:16:13,626 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:16:13,626 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:16:13,629 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:16:13,630 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:16:13,631 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:16:13,631 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:16:13,633 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:16:13,633 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:16:13,634 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:16:13,635 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:16:13,635 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:13,636 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:16:13,636 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:13,637 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:16:13,637 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:16:13,639 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 01:16:13,639 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:16:14,807 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:16:14,808 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]8372
2021-03-12 01:16:14,808 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:16:14,808 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:16:14,809 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:16:14,810 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:16:14,813 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:16:16,339 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:16:16,339 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:16:16,339 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:16:16,340 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:16:16,340 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:16:16,340 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:16:16,340 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:16:16,340 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:16:16,342 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:16:16,344 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:16:16,345 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:16:16,345 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:16:16,346 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:16,347 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:16:16,347 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:16,348 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:16:16,349 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 01:16:16,350 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-12 01:16:16,351 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:16:18,520 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:16:18,521 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]5432
2021-03-12 01:16:18,522 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:16:18,522 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:16:18,522 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:16:18,523 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:16:18,526 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:16:18,664 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-12 01:16:18,665 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-12 01:16:18,665 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-12 01:16:20,054 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:16:20,055 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:16:20,055 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:16:20,056 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:16:20,058 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:16:20,058 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:16:20,059 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:16:20,060 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:16:20,060 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:16:20,061 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:16:20,062 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:16:20,063 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:20,063 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:16:20,064 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:20,064 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:16:20,065 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:16:20,067 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2021-03-12 01:16:20,068 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:16:20,731 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-12 01:16:20,744 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retina version 1.0
2021-03-12 01:16:20,745 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retina version: 1.0
2021-03-12 01:16:20,747 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_SCALED_DOWN
2021-03-12 01:16:20,748 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:20,748 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:20,763 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retina unregistered.
2021-03-12 01:16:36,973 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retina=retina.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 01:16:37,000 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: retina.mar
2021-03-12 01:16:41,785 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag e9027262d6b943a8af6ad695b3b17d3e
2021-03-12 01:16:41,795 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-12 01:16:41,795 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-12 01:16:41,797 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-12 01:16:41,798 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-12 01:16:41,811 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-12 01:16:41,985 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:16:41,987 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]1616
2021-03-12 01:16:41,988 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:16:41,988 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-12 01:16:41,988 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:16:41,992 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:16:42,265 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-12 01:16:42,265 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-12 01:16:42,268 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-12 01:16:42,268 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-12 01:16:42,270 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-12 01:16:42,272 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:16:43,871 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:16:43,871 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:16:43,873 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:16:43,873 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:16:43,874 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:16:43,875 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:16:43,876 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:16:43,877 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:16:43,879 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:16:43,879 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:16:43,880 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:16:43,880 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:16:43,881 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:43,882 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:16:43,882 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:43,883 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:16:43,883 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 01:16:43,885 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 01:16:43,885 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:16:45,063 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:16:45,064 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]15384
2021-03-12 01:16:45,065 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:16:45,065 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:16:45,066 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:16:45,066 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:16:45,069 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:16:46,599 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:16:46,599 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:16:46,599 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:16:46,601 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:16:46,603 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:16:46,603 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:16:46,604 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:16:46,604 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:16:46,605 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:16:46,606 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:16:46,607 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:16:46,607 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:16:46,608 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:46,608 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:16:46,609 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:46,609 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:16:46,609 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 01:16:46,610 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 01:16:46,611 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:16:47,792 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:16:47,793 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]596
2021-03-12 01:16:47,794 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:16:47,794 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:16:47,795 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:16:47,795 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:16:47,799 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:16:49,330 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:16:49,331 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:16:49,446 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:16:50,362 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:16:50,363 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:16:50,363 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:16:50,364 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:16:50,364 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:16:50,366 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:16:50,366 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:16:50,367 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:16:50,368 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:16:50,369 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:50,370 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:16:50,370 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:50,371 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:16:50,371 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-12 01:16:50,372 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:16:50,538 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-12 01:16:50,543 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-12 01:16:50,544 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-12 01:16:52,545 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:16:52,547 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]8448
2021-03-12 01:16:52,549 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:16:52,549 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:16:52,550 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:16:52,551 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:16:52,554 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:16:52,611 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-12 01:16:52,623 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retina version 1.0
2021-03-12 01:16:52,624 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retina version: 1.0
2021-03-12 01:16:52,625 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_SCALED_DOWN
2021-03-12 01:16:52,626 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:52,626 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2021-03-12 01:16:52,627 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:52,627 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2021-03-12 01:16:52,628 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2021-03-12 01:16:52,629 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:16:52,629 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2021-03-12 01:16:52,630 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:16:52,631 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:16:52,632 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:16:52,632 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2021-03-12 01:16:52,632 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:16:52,746 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retina unregistered.
2021-03-12 01:30:03,439 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: logs\config\20210312011618667-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retinanet=retinanet.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 01:30:03,446 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20210312011618667-shutdown.cfg",
  "modelCount": 1,
  "created": 1615479378667,
  "models": {
    "retina": {
      "1.0": {
        "defaultVersion": true,
        "marName": "retina.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2021-03-12 01:30:03,454 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20210312011618667-shutdown.cfg
2021-03-12 01:30:03,456 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20210312011618667-shutdown.cfg validated successfully
2021-03-12 01:30:08,244 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 8382c911c9ec41baa5782a475ba34df7
2021-03-12 01:30:08,252 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retina
2021-03-12 01:30:08,253 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-12 01:30:08,254 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retina
2021-03-12 01:30:08,255 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retina loaded.
2021-03-12 01:30:08,256 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retina, count: 1
2021-03-12 01:30:08,269 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-12 01:30:08,448 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:30:08,449 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]1604
2021-03-12 01:30:08,450 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:30:08,450 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change null -> WORKER_STARTED
2021-03-12 01:30:08,451 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:30:08,455 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:30:08,724 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-12 01:30:08,724 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-12 01:30:08,728 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-12 01:30:08,728 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-12 01:30:08,728 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:30:08,730 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-12 01:30:10,387 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:30:10,388 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:30:10,389 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:30:10,389 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:30:10,390 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:30:10,391 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:30:10,392 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:30:10,393 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:30:10,395 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:30:10,395 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:30:10,396 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:30:10,397 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:30:10,397 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:30:10,398 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:30:10,398 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:30:10,399 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 01:30:10,399 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:30:10,400 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 01:30:10,402 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:30:11,587 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:30:11,588 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13824
2021-03-12 01:30:11,589 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:30:11,589 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:30:11,590 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:30:11,590 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:30:11,593 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:30:12,114 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-12 01:30:12,114 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-12 01:30:12,115 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-12 01:30:13,153 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:30:13,154 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:30:13,154 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:30:13,155 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:30:13,156 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:30:13,157 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:30:13,158 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:30:13,159 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retina, error: Worker died.
2021-03-12 01:30:13,160 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:30:13,160 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:30:13,161 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:30:13,162 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:30:13,162 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:30:13,163 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:30:13,163 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:30:13,164 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:30:13,165 [INFO ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 01:30:13,166 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:30:14,190 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-12 01:30:14,205 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retina version 1.0
2021-03-12 01:30:14,206 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retina version: 1.0
2021-03-12 01:30:14,207 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_STOPPED -> WORKER_SCALED_DOWN
2021-03-12 01:30:14,208 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:30:14,208 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:30:14,209 [DEBUG] Thread-0 org.pytorch.serve.job.Job - Waiting time ns: 0, Inference time ns: 2615998600
2021-03-12 01:30:14,209 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:30:14,210 [INFO ] W-9000-retina_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stderr
2021-03-12 01:30:14,210 [INFO ] W-9000-retina_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retina_1.0-stdout
2021-03-12 01:30:14,211 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:30:14,213 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2021-03-12 01:30:14,214 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2021-03-12 01:30:14,215 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retina_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2021-03-12 01:30:14,215 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stderr
2021-03-12 01:30:14,216 [WARN ] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retina_1.0-stdout
2021-03-12 01:30:14,216 [DEBUG] W-9000-retina_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2021-03-12 01:30:14,224 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retina unregistered.
2021-03-12 01:30:25,571 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retinanet=retinanet.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 01:30:25,600 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: retinanet.mar
2021-03-12 01:30:30,336 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 3082c4a9bd7c4a93956f04740a0a5b9c
2021-03-12 01:30:30,351 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retinanet
2021-03-12 01:30:30,352 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retinanet
2021-03-12 01:30:30,353 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retinanet loaded.
2021-03-12 01:30:30,354 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retinanet, count: 1
2021-03-12 01:30:30,368 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-12 01:30:30,546 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:30:30,548 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]12808
2021-03-12 01:30:30,549 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:30:30,549 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change null -> WORKER_STARTED
2021-03-12 01:30:30,549 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:30:30,553 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:30:30,830 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-12 01:30:30,831 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-12 01:30:30,833 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:30:30,834 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-12 01:30:30,834 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-12 01:30:30,835 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-12 01:30:32,428 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:30:32,428 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:30:32,430 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:30:32,431 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:30:32,432 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:30:32,433 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:30:32,433 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:30:32,434 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:30:32,436 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 01:30:32,437 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:30:32,437 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:30:32,438 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:30:32,438 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 01:30:32,439 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:30:32,439 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 01:30:32,440 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 01:30:32,440 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 01:30:32,442 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 01:30:32,442 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 01:30:33,616 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:30:33,617 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]7984
2021-03-12 01:30:33,618 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:30:33,618 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:30:33,619 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:30:33,619 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:30:33,623 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:30:35,164 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:30:35,164 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:30:35,164 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:30:35,166 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:30:35,167 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:30:35,168 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:30:35,169 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:30:35,169 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:30:35,170 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 01:30:35,171 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:30:35,172 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:30:35,173 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:30:35,173 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 01:30:35,174 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:30:35,174 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 01:30:35,175 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 01:30:35,176 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 01:30:35,177 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 01:30:35,178 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 01:30:36,351 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:30:36,353 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]1480
2021-03-12 01:30:36,353 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:30:36,354 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:30:36,354 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:30:36,355 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:30:36,358 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:30:37,909 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:30:37,910 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:30:37,910 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:30:37,912 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:30:37,913 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:30:37,914 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:30:37,914 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:30:37,915 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 01:30:37,916 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:30:37,917 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:30:37,917 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:30:37,918 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 01:30:37,919 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:30:37,919 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 01:30:37,921 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 01:30:37,922 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:30:37,922 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-12 01:30:37,923 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 01:30:40,094 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:30:40,095 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13208
2021-03-12 01:30:40,096 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:30:40,096 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:30:40,097 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:30:40,097 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:30:40,099 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:30:41,653 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:30:41,654 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:30:41,654 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:30:41,655 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:30:41,657 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:30:41,658 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:30:41,660 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:30:41,661 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 01:30:41,662 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:30:41,662 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:30:41,663 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:30:41,664 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 01:30:41,664 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:30:41,665 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 01:30:41,665 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 01:30:41,666 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:30:41,670 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2021-03-12 01:30:41,671 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 01:30:44,846 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 01:30:44,848 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]15716
2021-03-12 01:30:44,848 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 01:30:44,848 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 01:30:44,850 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 01:30:44,850 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 01:30:44,853 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 01:30:45,951 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-12 01:30:45,952 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-12 01:30:45,952 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-12 01:30:46,413 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 01:30:46,413 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 01:30:46,413 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 01:30:46,414 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 01:30:46,415 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 01:30:46,416 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 01:30:46,417 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 01:30:46,418 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 01:30:46,418 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 01:30:46,419 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 01:30:46,420 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 01:30:46,421 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 01:30:46,421 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 01:30:46,422 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 01:30:46,423 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 01:30:46,424 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 01:30:46,425 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2021-03-12 01:30:46,426 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 01:30:48,033 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-12 01:30:48,045 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retinanet version 1.0
2021-03-12 01:30:48,045 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retinanet version: 1.0
2021-03-12 01:30:48,047 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_SCALED_DOWN
2021-03-12 01:30:48,048 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 01:30:48,048 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 01:30:48,064 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retinanet unregistered.
2021-03-12 12:58:51,332 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: N/A
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 12:58:51,370 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-12 12:58:51,816 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-12 12:58:51,817 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-12 12:58:51,819 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-12 12:58:51,819 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-12 12:58:51,821 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-12 12:59:10,647 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-12 12:59:10,648 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-12 12:59:10,648 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-12 12:59:12,709 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-12 12:59:51,989 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: fastrcnn=fastrcnn.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 12:59:52,017 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: fastrcnn.mar
2021-03-12 12:59:57,353 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag e8e3405bd8ad48abaf5bb7bd3285c39f
2021-03-12 12:59:57,370 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model fastrcnn
2021-03-12 12:59:57,370 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model fastrcnn
2021-03-12 12:59:57,371 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model fastrcnn loaded.
2021-03-12 12:59:57,371 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: fastrcnn, count: 1
2021-03-12 12:59:57,385 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-12 12:59:57,582 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 12:59:57,584 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]15028
2021-03-12 12:59:57,585 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 12:59:57,585 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fastrcnn_1.0 State change null -> WORKER_STARTED
2021-03-12 12:59:57,586 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 12:59:57,590 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 12:59:57,851 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-12 12:59:57,851 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-12 12:59:57,854 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 12:59:57,855 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-12 12:59:57,856 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-12 12:59:57,857 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-12 13:00:02,330 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4443
2021-03-12 13:00:02,331 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fastrcnn_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2021-03-12 13:00:45,615 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2323
2021-03-12 13:00:45,618 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.job.Job - Waiting time ns: 147600, Backend time ns: 2326802400
2021-03-12 13:04:27,104 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: fasterrcnn=fasterrcnn.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 13:04:27,133 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: fasterrcnn.mar
2021-03-12 13:04:32,300 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 756f8a38608543508e6f8e3979241009
2021-03-12 13:04:32,314 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model fasterrcnn
2021-03-12 13:04:32,316 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model fasterrcnn
2021-03-12 13:04:32,317 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model fasterrcnn loaded.
2021-03-12 13:04:32,317 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: fasterrcnn, count: 1
2021-03-12 13:04:32,330 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-12 13:04:32,509 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 13:04:32,511 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]16268
2021-03-12 13:04:32,512 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 13:04:32,512 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fasterrcnn_1.0 State change null -> WORKER_STARTED
2021-03-12 13:04:32,512 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 13:04:32,516 [INFO ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 13:04:32,798 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-12 13:04:32,799 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-12 13:04:32,801 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 13:04:32,802 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-12 13:04:32,802 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-12 13:04:32,804 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-12 13:04:34,461 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 13:04:34,461 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 13:04:34,462 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 13:04:34,462 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 13:04:34,464 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 13:04:34,464 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 13:04:34,464 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 13:04:34,465 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 13:04:34,467 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: fasterrcnn, error: Worker died.
2021-03-12 13:04:34,467 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 13:04:34,468 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fasterrcnn_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 13:04:34,469 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 13:04:34,469 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fasterrcnn_1.0-stderr
2021-03-12 13:04:34,470 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 13:04:34,470 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fasterrcnn_1.0-stdout
2021-03-12 13:04:34,471 [INFO ] W-9000-fasterrcnn_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fasterrcnn_1.0-stderr
2021-03-12 13:04:34,471 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 13:04:34,476 [INFO ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 13:04:34,476 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fasterrcnn_1.0-stdout
2021-03-12 13:04:35,650 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 13:04:35,650 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]4084
2021-03-12 13:04:35,652 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 13:04:35,652 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fasterrcnn_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 13:04:35,653 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 13:04:35,653 [INFO ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 13:04:35,656 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 13:04:37,192 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 13:04:37,193 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 13:04:37,193 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 13:04:37,193 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 13:04:37,193 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 13:04:37,194 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 13:04:37,195 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 13:04:37,196 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: fasterrcnn, error: Worker died.
2021-03-12 13:04:37,196 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 13:04:37,197 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fasterrcnn_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 13:04:37,198 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 13:04:37,199 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fasterrcnn_1.0-stderr
2021-03-12 13:04:37,200 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 13:04:37,200 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fasterrcnn_1.0-stdout
2021-03-12 13:04:37,200 [INFO ] W-9000-fasterrcnn_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fasterrcnn_1.0-stderr
2021-03-12 13:04:37,202 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 13:04:37,205 [INFO ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 13:04:37,205 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fasterrcnn_1.0-stdout
2021-03-12 13:04:38,379 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 13:04:38,380 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]7688
2021-03-12 13:04:38,380 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 13:04:38,380 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fasterrcnn_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 13:04:38,381 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 13:04:38,382 [INFO ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 13:04:38,384 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 13:04:39,927 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 13:04:39,927 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 13:04:39,927 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 13:04:39,929 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 13:04:39,931 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 13:04:39,932 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 13:04:39,932 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 13:04:39,933 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: fasterrcnn, error: Worker died.
2021-03-12 13:04:39,934 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 13:04:39,935 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fasterrcnn_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 13:04:39,936 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 13:04:39,936 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fasterrcnn_1.0-stderr
2021-03-12 13:04:39,937 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 13:04:39,937 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fasterrcnn_1.0-stdout
2021-03-12 13:04:39,938 [INFO ] W-9000-fasterrcnn_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fasterrcnn_1.0-stderr
2021-03-12 13:04:39,938 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 13:04:39,939 [INFO ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-12 13:04:39,940 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fasterrcnn_1.0-stdout
2021-03-12 13:04:42,122 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 13:04:42,123 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]9012
2021-03-12 13:04:42,124 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 13:04:42,124 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fasterrcnn_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 13:04:42,124 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 13:04:42,124 [INFO ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 13:04:42,126 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 13:04:43,694 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 13:04:43,694 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 13:04:43,694 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 13:04:43,696 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 13:04:43,696 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 13:04:43,697 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 13:04:43,698 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 13:04:43,699 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: fasterrcnn, error: Worker died.
2021-03-12 13:04:43,699 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 13:04:43,700 [DEBUG] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fasterrcnn_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 13:04:43,701 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 13:04:43,701 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fasterrcnn_1.0-stderr
2021-03-12 13:04:43,702 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 13:04:43,702 [WARN ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-fasterrcnn_1.0-stdout
2021-03-12 13:04:43,703 [INFO ] W-9000-fasterrcnn_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fasterrcnn_1.0-stderr
2021-03-12 13:04:43,703 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 13:04:43,704 [INFO ] W-9000-fasterrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2021-03-12 13:04:43,705 [INFO ] W-9000-fasterrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-fasterrcnn_1.0-stdout
2021-03-12 15:01:14,442 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: logs\config\20210312013012116-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retinanet=retinanet.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 15:01:14,449 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20210312013012116-shutdown.cfg",
  "modelCount": 1,
  "created": 1615480212116,
  "models": {
    "retina": {
      "1.0": {
        "defaultVersion": true,
        "marName": "retina.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2021-03-12 15:01:14,458 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20210312013012116-shutdown.cfg
2021-03-12 15:01:14,460 [ERROR] main org.pytorch.serve.snapshot.SnapshotManager - Model archive file for model retina, version 1.0 not found in model store
2021-03-12 15:01:16,507 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-12 15:01:38,832 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: retinanet=retinanet.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-12 15:01:38,862 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: retinanet.mar
2021-03-12 15:01:43,719 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag b04493c8ec3047f183dfc8de9419a0ca
2021-03-12 15:01:43,734 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model retinanet
2021-03-12 15:01:43,735 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model retinanet
2021-03-12 15:01:43,738 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model retinanet loaded.
2021-03-12 15:01:43,738 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: retinanet, count: 1
2021-03-12 15:01:43,751 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-12 15:01:43,937 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 15:01:43,939 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]3108
2021-03-12 15:01:43,940 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 15:01:43,941 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change null -> WORKER_STARTED
2021-03-12 15:01:43,941 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 15:01:43,945 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 15:01:44,221 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-12 15:01:44,221 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-12 15:01:44,223 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 15:01:44,225 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-12 15:01:44,225 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-12 15:01:44,227 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-12 15:01:46,829 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 15:01:46,829 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 15:01:46,830 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 15:01:46,830 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 15:01:46,831 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 15:01:46,831 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 15:01:46,832 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 15:01:46,832 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 15:01:46,833 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 15:01:46,833 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-12 15:01:46,834 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-12 15:01:46,834 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-12 15:01:46,835 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-12 15:01:46,836 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:01:46,836 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-12 15:01:46,837 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:01:46,837 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-12 15:01:46,838 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-12 15:01:46,838 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-12 15:01:46,839 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-12 15:01:46,839 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-12 15:01:46,840 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-12 15:01:46,840 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-12 15:01:46,845 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-12 15:01:46,853 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-12 15:01:46,866 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 15:01:46,867 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 15:01:46,868 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 15:01:46,871 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 15:01:46,871 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 15:01:46,872 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 15:01:46,873 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 15:01:46,874 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 15:01:46,875 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 15:01:46,875 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 15:01:48,059 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 15:01:48,060 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13124
2021-03-12 15:01:48,061 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 15:01:48,061 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 15:01:48,062 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 15:01:48,062 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 15:01:48,065 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 15:01:50,185 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 15:01:50,185 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 15:01:50,186 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 15:01:50,187 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 15:01:50,187 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 15:01:50,188 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 15:01:50,188 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 15:01:50,189 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 15:01:50,189 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 15:01:50,190 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-12 15:01:50,190 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-12 15:01:50,191 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-12 15:01:50,191 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-12 15:01:50,192 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:01:50,192 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-12 15:01:50,193 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:01:50,194 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-12 15:01:50,194 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-12 15:01:50,195 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-12 15:01:50,195 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-12 15:01:50,196 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-12 15:01:50,196 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-12 15:01:50,197 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-12 15:01:50,199 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-12 15:01:50,211 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-12 15:01:50,224 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 15:01:50,225 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 15:01:50,225 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 15:01:50,226 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 15:01:50,227 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 15:01:50,228 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 15:01:50,228 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 15:01:50,229 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-12 15:01:50,230 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 15:01:50,230 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 15:01:51,419 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 15:01:51,421 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]12240
2021-03-12 15:01:51,421 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 15:01:51,421 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 15:01:51,422 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 15:01:51,423 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 15:01:51,426 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 15:01:53,625 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 15:01:53,625 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 15:01:53,626 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 15:01:53,627 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 15:01:53,627 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 15:01:53,628 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 15:01:53,629 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 15:01:53,629 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 15:01:53,630 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 15:01:53,630 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-12 15:01:53,631 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-12 15:01:53,633 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-12 15:01:53,633 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-12 15:01:53,634 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:01:53,634 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-12 15:01:53,635 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:01:53,636 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-12 15:01:53,637 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-12 15:01:53,664 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-12 15:01:53,667 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-12 15:01:53,668 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-12 15:01:53,668 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-12 15:01:53,669 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-12 15:01:53,674 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-12 15:01:53,694 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 15:01:53,694 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-12 15:01:53,695 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 15:01:53,697 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 15:01:53,699 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 15:01:53,700 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 15:01:53,701 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 15:01:53,701 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 15:01:53,702 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-12 15:01:53,704 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 15:01:53,704 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 15:01:55,896 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 15:01:55,897 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]16176
2021-03-12 15:01:55,898 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 15:01:55,898 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 15:01:55,898 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 15:01:55,899 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 15:01:55,902 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 15:01:58,028 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 15:01:58,028 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 15:01:58,029 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 15:01:58,030 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 15:01:58,030 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 15:01:58,030 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 15:01:58,031 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 15:01:58,031 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 15:01:58,032 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 15:01:58,032 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-12 15:01:58,033 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-12 15:01:58,033 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-12 15:01:58,034 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-12 15:01:58,035 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:01:58,035 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-12 15:01:58,035 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:01:58,036 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-12 15:01:58,037 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-12 15:01:58,037 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-12 15:01:58,038 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-12 15:01:58,042 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-12 15:01:58,042 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-12 15:01:58,043 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-12 15:01:58,045 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-12 15:01:58,054 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-12 15:01:58,069 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 15:01:58,069 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 15:01:58,070 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 15:01:58,071 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 15:01:58,072 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 15:01:58,072 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 15:01:58,073 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 15:01:58,075 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 15:01:58,075 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 15:01:58,075 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2021-03-12 15:02:01,260 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 15:02:01,261 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]9348
2021-03-12 15:02:01,262 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 15:02:01,262 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 15:02:01,262 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 15:02:01,263 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 15:02:01,266 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 15:02:03,384 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 15:02:03,384 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 15:02:03,385 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 15:02:03,386 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 15:02:03,386 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 15:02:03,387 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 15:02:03,387 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 15:02:03,388 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 15:02:03,388 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 15:02:03,389 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-12 15:02:03,389 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-12 15:02:03,390 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-12 15:02:03,390 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-12 15:02:03,391 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:02:03,392 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-12 15:02:03,392 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:02:03,393 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-12 15:02:03,393 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-12 15:02:03,394 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-12 15:02:03,394 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-12 15:02:03,395 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-12 15:02:03,397 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-12 15:02:03,397 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-12 15:02:03,399 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-12 15:02:03,411 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-12 15:02:03,422 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 15:02:03,423 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 15:02:03,423 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 15:02:03,424 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 15:02:03,425 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 15:02:03,426 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 15:02:03,427 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 15:02:03,428 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 15:02:03,428 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 15:02:03,429 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2021-03-12 15:02:08,609 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-12 15:02:08,610 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]3936
2021-03-12 15:02:08,611 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-12 15:02:08,611 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-12 15:02:08,611 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-12 15:02:08,612 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-12 15:02:08,614 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-12 15:02:10,702 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-12 15:02:10,702 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-12 15:02:10,703 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 182, in <module>
2021-03-12 15:02:10,704 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-12 15:02:10,704 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 154, in run_server
2021-03-12 15:02:10,705 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-12 15:02:10,705 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 116, in handle_connection
2021-03-12 15:02:10,706 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-12 15:02:10,706 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages\ts\model_service_worker.py", line 89, in load_model
2021-03-12 15:02:10,707 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-12 15:02:10,707 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\model_loader.py", line 104, in load
2021-03-12 15:02:10,708 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     initialize_fn(service.context)
2021-03-12 15:02:10,708 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\object_detector.py", line 22, in initialize
2021-03-12 15:02:10,709 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:02:10,710 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\vision_handler.py", line 20, in initialize
2021-03-12 15:02:10,710 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     super().initialize(context)
2021-03-12 15:02:10,711 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 67, in initialize
2021-03-12 15:02:10,711 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)
2021-03-12 15:02:10,712 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\ts\torch_handler\base_handler.py", line 127, in _load_pickled_model
2021-03-12 15:02:10,713 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model.load_state_dict(state_dict)
2021-03-12 15:02:10,714 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "c:\users\nyk70\appdata\local\programs\python\python38\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict
2021-03-12 15:02:10,715 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
2021-03-12 15:02:10,715 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError: Error(s) in loading state_dict for RetinaNetObjectDetector:
2021-03-12 15:02:10,716 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Missing key(s) in state_dict: "backbone.body.conv1.weight", "backbone.body.bn1.weight", "backbone.body.bn1.bias", "backbone.body.bn1.running_mean", "backbone.body.bn1.running_var", "backbone.body.layer1.0.conv1.weight", "backbone.body.layer1.0.bn1.weight", "backbone.body.layer1.0.bn1.bias", "backbone.body.layer1.0.bn1.running_mean", "backbone.body.layer1.0.bn1.running_var", "backbone.body.layer1.0.conv2.weight", "backbone.body.layer1.0.bn2.weight", "backbone.body.layer1.0.bn2.bias", "backbone.body.layer1.0.bn2.running_mean", "backbone.body.layer1.0.bn2.running_var", "backbone.body.layer1.0.conv3.weight", "backbone.body.layer1.0.bn3.weight", "backbone.body.layer1.0.bn3.bias", "backbone.body.layer1.0.bn3.running_mean", "backbone.body.layer1.0.bn3.running_var", "backbone.body.layer1.0.downsample.0.weight", "backbone.body.layer1.0.downsample.1.weight", "backbone.body.layer1.0.downsample.1.bias", "backbone.body.layer1.0.downsample.1.running_mean", "backbone.body.layer1.0.downsample.1.running_var", "backbone.body.layer1.1.conv1.weight", "backbone.body.layer1.1.bn1.weight", "backbone.body.layer1.1.bn1.bias", "backbone.body.layer1.1.bn1.running_mean", "backbone.body.layer1.1.bn1.running_var", "backbone.body.layer1.1.conv2.weight", "backbone.body.layer1.1.bn2.weight", "backbone.body.layer1.1.bn2.bias", "backbone.body.layer1.1.bn2.running_mean", "backbone.body.layer1.1.bn2.running_var", "backbone.body.layer1.1.conv3.weight", "backbone.body.layer1.1.bn3.weight", "backbone.body.layer1.1.bn3.bias", "backbone.body.layer1.1.bn3.running_mean", "backbone.body.layer1.1.bn3.running_var", "backbone.body.layer1.2.conv1.weight", "backbone.body.layer1.2.bn1.weight", "backbone.body.layer1.2.bn1.bias", "backbone.body.layer1.2.bn1.running_mean", "backbone.body.layer1.2.bn1.running_var", "backbone.body.layer1.2.conv2.weight", "backbone.body.layer1.2.bn2.weight", "backbone.body.layer1.2.bn2.bias", "backbone.body.layer1.2.bn2.running_mean", "backbone.body.layer1.2.bn2.running_var", "backbone.body.layer1.2.conv3.weight", "backbone.body.layer1.2.bn3.weight", "backbone.body.layer1.2.bn3.bias", "backbone.body.layer1.2.bn3.running_mean", "backbone.body.layer1.2.bn3.running_var", "backbone.body.layer2.0.conv1.weight", "backbone.body.layer2.0.bn1.weight", "backbone.body.layer2.0.bn1.bias", "backbone.body.layer2.0.bn1.running_mean", "backbone.body.layer2.0.bn1.running_var", "backbone.body.layer2.0.conv2.weight", "backbone.body.layer2.0.bn2.weight", "backbone.body.layer2.0.bn2.bias", "backbone.body.layer2.0.bn2.running_mean", "backbone.body.layer2.0.bn2.running_var", "backbone.body.layer2.0.conv3.weight", "backbone.body.layer2.0.bn3.weight", "backbone.body.layer2.0.bn3.bias", "backbone.body.layer2.0.bn3.running_mean", "backbone.body.layer2.0.bn3.running_var", "backbone.body.layer2.0.downsample.0.weight", "backbone.body.layer2.0.downsample.1.weight", "backbone.body.layer2.0.downsample.1.bias", "backbone.body.layer2.0.downsample.1.running_mean", "backbone.body.layer2.0.downsample.1.running_var", "backbone.body.layer2.1.conv1.weight", "backbone.body.layer2.1.bn1.weight", "backbone.body.layer2.1.bn1.bias", "backbone.body.layer2.1.bn1.running_mean", "backbone.body.layer2.1.bn1.running_var", "backbone.body.layer2.1.conv2.weight", "backbone.body.layer2.1.bn2.weight", "backbone.body.layer2.1.bn2.bias", "backbone.body.layer2.1.bn2.running_mean", "backbone.body.layer2.1.bn2.running_var", "backbone.body.layer2.1.conv3.weight", "backbone.body.layer2.1.bn3.weight", "backbone.body.layer2.1.bn3.bias", "backbone.body.layer2.1.bn3.running_mean", "backbone.body.layer2.1.bn3.running_var", "backbone.body.layer2.2.conv1.weight", "backbone.body.layer2.2.bn1.weight", "backbone.body.layer2.2.bn1.bias", "backbone.body.layer2.2.bn1.running_mean", "backbone.body.layer2.2.bn1.running_var", "backbone.body.layer2.2.conv2.weight", "backbone.body.layer2.2.bn2.weight", "backbone.body.layer2.2.bn2.bias", "backbone.body.layer2.2.bn2.running_mean", "backbone.body.layer2.2.bn2.running_var", "backbone.body.layer2.2.conv3.weight", "backbone.body.layer2.2.bn3.weight", "backbone.body.layer2.2.bn3.bias", "backbone.body.layer2.2.bn3.running_mean", "backbone.body.layer2.2.bn3.running_var", "backbone.body.layer2.3.conv1.weight", "backbone.body.layer2.3.bn1.weight", "backbone.body.layer2.3.bn1.bias", "backbone.body.layer2.3.bn1.running_mean", "backbone.body.layer2.3.bn1.running_var", "backbone.body.layer2.3.conv2.weight", "backbone.body.layer2.3.bn2.weight", "backbone.body.layer2.3.bn2.bias", "backbone.body.layer2.3.bn2.running_mean", "backbone.body.layer2.3.bn2.running_var", "backbone.body.layer2.3.conv3.weight", "backbone.body.layer2.3.bn3.weight", "backbone.body.layer2.3.bn3.bias", "backbone.body.layer2.3.bn3.running_mean", "backbone.body.layer2.3.bn3.running_var", "backbone.body.layer3.0.conv1.weight", "backbone.body.layer3.0.bn1.weight", "backbone.body.layer3.0.bn1.bias", "backbone.body.layer3.0.bn1.running_mean", "backbone.body.layer3.0.bn1.running_var", "backbone.body.layer3.0.conv2.weight", "backbone.body.layer3.0.bn2.weight", "backbone.body.layer3.0.bn2.bias", "backbone.body.layer3.0.bn2.running_mean", "backbone.body.layer3.0.bn2.running_var", "backbone.body.layer3.0.conv3.weight", "backbone.body.layer3.0.bn3.weight", "backbone.body.layer3.0.bn3.bias", "backbone.body.layer3.0.bn3.running_mean", "backbone.body.layer3.0.bn3.running_var", "backbone.body.layer3.0.downsample.0.weight", "backbone.body.layer3.0.downsample.1.weight", "backbone.body.layer3.0.downsample.1.bias", "backbone.body.layer3.0.downsample.1.running_mean", "backbone.body.layer3.0.downsample.1.running_var", "backbone.body.layer3.1.conv1.weight", "backbone.body.layer3.1.bn1.weight", "backbone.body.layer3.1.bn1.bias", "backbone.body.layer3.1.bn1.running_mean", "backbone.body.layer3.1.bn1.running_var", "backbone.body.layer3.1.conv2.weight", "backbone.body.layer3.1.bn2.weight", "backbone.body.layer3.1.bn2.bias", "backbone.body.layer3.1.bn2.running_mean", "backbone.body.layer3.1.bn2.running_var", "backbone.body.layer3.1.conv3.weight", "backbone.body.layer3.1.bn3.weight", "backbone.body.layer3.1.bn3.bias", "backbone.body.layer3.1.bn3.running_mean", "backbone.body.layer3.1.bn3.running_var", "backbone.body.layer3.2.conv1.weight", "backbone.body.layer3.2.bn1.weight", "backbone.body.layer3.2.bn1.bias", "backbone.body.layer3.2.bn1.running_mean", "backbone.body.layer3.2.bn1.running_var", "backbone.body.layer3.2.conv2.weight", "backbone.body.layer3.2.bn2.weight", "backbone.body.layer3.2.bn2.bias", "backbone.body.layer3.2.bn2.running_mean", "backbone.body.layer3.2.bn2.running_var", "backbone.body.layer3.2.conv3.weight", "backbone.body.layer3.2.bn3.weight", "backbone.body.layer3.2.bn3.bias", "backbone.body.layer3.2.bn3.running_mean", "backbone.body.layer3.2.bn3.running_var", "backbone.body.layer3.3.conv1.weight", "backbone.body.layer3.3.bn1.weight", "backbone.body.layer3.3.bn1.bias", "backbone.body.layer3.3.bn1.running_mean", "backbone.body.layer3.3.bn1.running_var", "backbone.body.layer3.3.conv2.weight", "backbone.body.layer3.3.bn2.weight", "backbone.body.layer3.3.bn2.bias", "backbone.body.layer3.3.bn2.running_mean", "backbone.body.layer3.3.bn2.running_var", "backbone.body.layer3.3.conv3.weight", "backbone.body.layer3.3.bn3.weight", "backbone.body.layer3.3.bn3.bias", "backbone.body.layer3.3.bn3.running_mean", "backbone.body.layer3.3.bn3.running_var", "backbone.body.layer3.4.conv1.weight", "backbone.body.layer3.4.bn1.weight", "backbone.body.layer3.4.bn1.bias", "backbone.body.layer3.4.bn1.running_mean", "backbone.body.layer3.4.bn1.running_var", "backbone.body.layer3.4.conv2.weight", "backbone.body.layer3.4.bn2.weight", "backbone.body.layer3.4.bn2.bias", "backbone.body.layer3.4.bn2.running_mean", "backbone.body.layer3.4.bn2.running_var", "backbone.body.layer3.4.conv3.weight", "backbone.body.layer3.4.bn3.weight", "backbone.body.layer3.4.bn3.bias", "backbone.body.layer3.4.bn3.running_mean", "backbone.body.layer3.4.bn3.running_var", "backbone.body.layer3.5.conv1.weight", "backbone.body.layer3.5.bn1.weight", "backbone.body.layer3.5.bn1.bias", "backbone.body.layer3.5.bn1.running_mean", "backbone.body.layer3.5.bn1.running_var", "backbone.body.layer3.5.conv2.weight", "backbone.body.layer3.5.bn2.weight", "backbone.body.layer3.5.bn2.bias", "backbone.body.layer3.5.bn2.running_mean", "backbone.body.layer3.5.bn2.running_var", "backbone.body.layer3.5.conv3.weight", "backbone.body.layer3.5.bn3.weight", "backbone.body.layer3.5.bn3.bias", "backbone.body.layer3.5.bn3.running_mean", "backbone.body.layer3.5.bn3.running_var", "backbone.body.layer4.0.conv1.weight", "backbone.body.layer4.0.bn1.weight", "backbone.body.layer4.0.bn1.bias", "backbone.body.layer4.0.bn1.running_mean", "backbone.body.layer4.0.bn1.running_var", "backbone.body.layer4.0.conv2.weight", "backbone.body.layer4.0.bn2.weight", "backbone.body.layer4.0.bn2.bias", "backbone.body.layer4.0.bn2.running_mean", "backbone.body.layer4.0.bn2.running_var", "backbone.body.layer4.0.conv3.weight", "backbone.body.layer4.0.bn3.weight", "backbone.body.layer4.0.bn3.bias", "backbone.body.layer4.0.bn3.running_mean", "backbone.body.layer4.0.bn3.running_var", "backbone.body.layer4.0.downsample.0.weight", "backbone.body.layer4.0.downsample.1.weight", "backbone.body.layer4.0.downsample.1.bias", "backbone.body.layer4.0.downsample.1.running_mean", "backbone.body.layer4.0.downsample.1.running_var", "backbone.body.layer4.1.conv1.weight", "backbone.body.layer4.1.bn1.weight", "backbone.body.layer4.1.bn1.bias", "backbone.body.layer4.1.bn1.running_mean", "backbone.body.layer4.1.bn1.running_var", "backbone.body.layer4.1.conv2.weight", "backbone.body.layer4.1.bn2.weight", "backbone.body.layer4.1.bn2.bias", "backbone.body.layer4.1.bn2.running_mean", "backbone.body.layer4.1.bn2.running_var", "backbone.body.layer4.1.conv3.weight", "backbone.body.layer4.1.bn3.weight", "backbone.body.layer4.1.bn3.bias", "backbone.body.layer4.1.bn3.running_mean", "backbone.body.layer4.1.bn3.running_var", "backbone.body.layer4.2.conv1.weight", "backbone.body.layer4.2.bn1.weight", "backbone.body.layer4.2.bn1.bias", "backbone.body.layer4.2.bn1.running_mean", "backbone.body.layer4.2.bn1.running_var", "backbone.body.layer4.2.conv2.weight", "backbone.body.layer4.2.bn2.weight", "backbone.body.layer4.2.bn2.bias", "backbone.body.layer4.2.bn2.running_mean", "backbone.body.layer4.2.bn2.running_var", "backbone.body.layer4.2.conv3.weight", "backbone.body.layer4.2.bn3.weight", "backbone.body.layer4.2.bn3.bias", "backbone.body.layer4.2.bn3.running_mean", "backbone.body.layer4.2.bn3.running_var", "backbone.fpn.inner_blocks.0.weight", "backbone.fpn.inner_blocks.0.bias", "backbone.fpn.inner_blocks.1.weight", "backbone.fpn.inner_blocks.1.bias", "backbone.fpn.inner_blocks.2.weight", "backbone.fpn.inner_blocks.2.bias", "backbone.fpn.inner_blocks.3.weight", "backbone.fpn.inner_blocks.3.bias", "backbone.fpn.layer_blocks.0.weight", "backbone.fpn.layer_blocks.0.bias", "backbone.fpn.layer_blocks.1.weight", "backbone.fpn.layer_blocks.1.bias", "backbone.fpn.layer_blocks.2.weight", "backbone.fpn.layer_blocks.2.bias", "backbone.fpn.layer_blocks.3.weight", "backbone.fpn.layer_blocks.3.bias", "head.classification_head.conv.0.weight", "head.classification_head.conv.0.bias", "head.classification_head.conv.2.weight", "head.classification_head.conv.2.bias", "head.classification_head.conv.4.weight", "head.classification_head.conv.4.bias", "head.classification_head.conv.6.weight", "head.classification_head.conv.6.bias", "head.classification_head.cls_logits.weight", "head.classification_head.cls_logits.bias", "head.regression_head.conv.0.weight", "head.regression_head.conv.0.bias", "head.regression_head.conv.2.weight", "head.regression_head.conv.2.bias", "head.regression_head.conv.4.weight", "head.regression_head.conv.4.bias", "head.regression_head.conv.6.weight", "head.regression_head.conv.6.bias", "head.regression_head.bbox_reg.weight", "head.regression_head.bbox_reg.bias". 
2021-03-12 15:02:10,723 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 	Unexpected key(s) in state_dict: "model". 
2021-03-12 15:02:10,738 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-12 15:02:10,738 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-12 15:02:10,739 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-12 15:02:10,740 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: retinanet, error: Worker died.
2021-03-12 15:02:10,740 [DEBUG] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-12 15:02:10,741 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 15:02:10,741 [WARN ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 15:02:10,742 [INFO ] W-9000-retinanet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2021-03-12 15:02:10,742 [INFO ] W-9000-retinanet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stderr
2021-03-12 15:02:10,742 [INFO ] W-9000-retinanet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-retinanet_1.0-stdout
2021-03-12 15:02:15,323 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-12 15:02:15,323 [INFO ] nioEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-12 15:02:15,323 [INFO ] nioEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-12 15:02:17,381 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-12 15:02:17,394 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model retinanet version 1.0
2021-03-12 15:02:17,394 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: retinanet version: 1.0
2021-03-12 15:02:17,394 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-retinanet_1.0 State change WORKER_STOPPED -> WORKER_SCALED_DOWN
2021-03-12 15:02:17,396 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stderr
2021-03-12 15:02:17,397 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-retinanet_1.0-stdout
2021-03-12 15:02:17,412 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model retinanet unregistered.
2021-03-23 14:43:35,472 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: C:\Users\nyk70\AppData\Local\Programs\Python\Python38\Lib\site-packages
Current directory: C:\Users\nyk70\Documents\git\torchserve-test\serve
Temp directory: C:\Users\nyk70\AppData\Local\Temp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 4080 M
Python executable: c:\users\nyk70\appdata\local\programs\python\python38\python.exe
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\nyk70\Documents\git\torchserve-test\serve\model_store
Initial Models: fastrcnn=fastrcnn.mar
Log dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Metrics dir: C:\Users\nyk70\Documents\git\torchserve-test\serve\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-23 14:43:35,508 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: fastrcnn.mar
2021-03-23 14:43:40,801 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 44edbb89fd8140e8a0ba7e2effb4a061
2021-03-23 14:43:40,821 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model fastrcnn
2021-03-23 14:43:40,821 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model fastrcnn
2021-03-23 14:43:40,822 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model fastrcnn loaded.
2021-03-23 14:43:40,823 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: fastrcnn, count: 1
2021-03-23 14:43:40,843 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2021-03-23 14:43:41,040 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: None
2021-03-23 14:43:41,041 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]1968
2021-03-23 14:43:41,043 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-23 14:43:41,043 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fastrcnn_1.0 State change null -> WORKER_STARTED
2021-03-23 14:43:41,043 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-23 14:43:41,049 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2021-03-23 14:43:41,329 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-23 14:43:41,329 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2021-03-23 14:43:41,333 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-23 14:43:41,333 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2021-03-23 14:43:41,335 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-23 14:43:41,336 [INFO ] W-9000-fastrcnn_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: ('127.0.0.1', 9000).
2021-03-23 14:43:45,543 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4172
2021-03-23 14:43:45,544 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-fastrcnn_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2021-03-23 14:44:07,109 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2289
2021-03-23 14:44:23,622 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.job.Job - Waiting time ns: 137900, Backend time ns: 18803036000
2021-03-23 14:44:31,813 [INFO ] W-9000-fastrcnn_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2215
2021-03-23 14:44:31,815 [DEBUG] W-9000-fastrcnn_1.0 org.pytorch.serve.job.Job - Waiting time ns: 139800, Backend time ns: 2217061800
2021-03-23 17:41:30,429 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.1
TS Home: /home/linux-lyj/.local/lib/python3.6/site-packages
Current directory: /home/linux-lyj/git/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 3988 M
Python executable: /usr/bin/python3
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/linux-lyj/git/torchserve/model_store
Initial Models: detectron2=detectron2.mar
Log dir: /home/linux-lyj/git/torchserve/logs
Metrics dir: /home/linux-lyj/git/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-23 17:41:30,520 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: detectron2.mar
2021-03-23 17:41:30,523 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: detectron2.mar
org.pytorch.serve.archive.ModelNotFoundException: Model not found at: detectron2.mar
	at org.pytorch.serve.archive.ModelArchive.downloadModel(ModelArchive.java:86)
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:135)
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:112)
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:227)
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:327)
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:114)
	at org.pytorch.serve.ModelServer.main(ModelServer.java:95)
2021-03-23 17:41:30,528 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2021-03-23 17:41:30,618 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-23 17:41:30,619 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2021-03-23 17:41:30,619 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-23 17:41:30,620 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2021-03-23 17:41:30,620 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-23 17:41:35,935 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-23 17:41:35,938 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-23 17:41:35,938 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-23 17:41:37,976 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-23 17:42:33,158 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.1
TS Home: /home/linux-lyj/.local/lib/python3.6/site-packages
Current directory: /home/linux-lyj/git/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 3988 M
Python executable: /usr/bin/python3
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/linux-lyj/git/torchserve/model_store
Initial Models: detectron2=detectron2.mar
Log dir: /home/linux-lyj/git/torchserve/logs
Metrics dir: /home/linux-lyj/git/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-23 17:42:33,188 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: detectron2.mar
2021-03-23 17:42:43,036 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 90ecb89bfb914eba91b09b3ff2ae6903
2021-03-23 17:42:43,136 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model detectron2
2021-03-23 17:42:43,136 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model detectron2
2021-03-23 17:42:43,136 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model detectron2 loaded.
2021-03-23 17:42:43,137 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: detectron2, count: 1
2021-03-23 17:42:43,433 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2021-03-23 17:42:43,485 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-23 17:42:43,485 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2021-03-23 17:42:43,486 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-23 17:42:43,486 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2021-03-23 17:42:43,487 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-23 17:42:43,623 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9000
2021-03-23 17:42:43,623 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13790
2021-03-23 17:42:43,623 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-23 17:42:43,623 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.9
2021-03-23 17:42:43,624 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change null -> WORKER_STARTED
2021-03-23 17:42:43,647 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2021-03-23 17:42:43,661 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9000.
2021-03-23 17:42:43,724 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-23 17:42:43,724 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-23 17:42:43,724 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 81, in load
2021-03-23 17:42:43,724 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module, function_name = self._load_handler_file(handler)
2021-03-23 17:42:43,725 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 115, in _load_handler_file
2021-03-23 17:42:43,725 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module = importlib.import_module(module_name)
2021-03-23 17:42:43,725 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/usr/lib/python3.6/importlib/__init__.py", line 126, in import_module
2021-03-23 17:42:43,725 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     return _bootstrap._gcd_import(name[level:], package, level)
2021-03-23 17:42:43,725 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-23 17:42:43,725 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 994, in _gcd_import
2021-03-23 17:42:43,725 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 971, in _find_and_load
2021-03-23 17:42:43,725 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
2021-03-23 17:42:43,725 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-23 17:42:43,725 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap_external>", line 678, in exec_module
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/tmp/models/90ecb89bfb914eba91b09b3ff2ae6903/dt2_handler.py", line 2, in <module>
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     import detectron2
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ModuleNotFoundError: No module named 'detectron2'
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_service_worker.py", line 182, in <module>
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_service_worker.py", line 154, in run_server
2021-03-23 17:42:43,726 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_service_worker.py", line 116, in handle_connection
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_service_worker.py", line 89, in load_model
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 83, in load
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module = self._load_default_handler(handler)
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 120, in _load_default_handler
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module = importlib.import_module(module_name, 'ts.torch_handler')
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/usr/lib/python3.6/importlib/__init__.py", line 126, in import_module
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     return _bootstrap._gcd_import(name[level:], package, level)
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 994, in _gcd_import
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 971, in _find_and_load
2021-03-23 17:42:43,727 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 941, in _find_and_load_unlocked
2021-03-23 17:42:43,728 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2021-03-23 17:42:43,728 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 994, in _gcd_import
2021-03-23 17:42:43,728 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 971, in _find_and_load
2021-03-23 17:42:43,728 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 953, in _find_and_load_unlocked
2021-03-23 17:42:43,728 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ModuleNotFoundError: No module named 'ts.torch_handler.dt2_handler'
2021-03-23 17:42:43,728 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-23 17:42:43,730 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: detectron2, error: Worker died.
2021-03-23 17:42:43,730 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-23 17:42:43,730 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stderr
2021-03-23 17:42:43,730 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stdout
2021-03-23 17:42:43,736 [INFO ] W-9000-detectron2_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stderr
2021-03-23 17:42:43,736 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stdout
2021-03-23 17:42:43,736 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-23 17:42:44,841 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9000
2021-03-23 17:42:44,841 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13808
2021-03-23 17:42:44,841 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-23 17:42:44,841 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.9
2021-03-23 17:42:44,841 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-23 17:42:44,841 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2021-03-23 17:42:44,843 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9000.
2021-03-23 17:42:44,854 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-23 17:42:44,855 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-23 17:42:44,855 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 81, in load
2021-03-23 17:42:44,855 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module, function_name = self._load_handler_file(handler)
2021-03-23 17:42:44,855 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: detectron2, error: Worker died.
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 115, in _load_handler_file
2021-03-23 17:42:44,855 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module = importlib.import_module(module_name)
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/usr/lib/python3.6/importlib/__init__.py", line 126, in import_module
2021-03-23 17:42:44,855 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stderr
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     return _bootstrap._gcd_import(name[level:], package, level)
2021-03-23 17:42:44,855 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stdout
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 994, in _gcd_import
2021-03-23 17:42:44,855 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-03-23 17:42:44,856 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stdout
2021-03-23 17:42:44,856 [INFO ] W-9000-detectron2_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stderr
2021-03-23 17:42:45,925 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9000
2021-03-23 17:42:45,925 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13813
2021-03-23 17:42:45,925 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-23 17:42:45,925 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.9
2021-03-23 17:42:45,925 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-23 17:42:45,926 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2021-03-23 17:42:45,927 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9000.
2021-03-23 17:42:45,938 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-23 17:42:45,938 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-23 17:42:45,939 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 81, in load
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module, function_name = self._load_handler_file(handler)
2021-03-23 17:42:45,939 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 115, in _load_handler_file
2021-03-23 17:42:45,939 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module = importlib.import_module(module_name)
2021-03-23 17:42:45,939 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: detectron2, error: Worker died.
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/usr/lib/python3.6/importlib/__init__.py", line 126, in import_module
2021-03-23 17:42:45,939 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     return _bootstrap._gcd_import(name[level:], package, level)
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 994, in _gcd_import
2021-03-23 17:42:45,939 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stderr
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 971, in _find_and_load
2021-03-23 17:42:45,939 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stdout
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
2021-03-23 17:42:45,939 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2021-03-23 17:42:45,940 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stdout
2021-03-23 17:42:45,940 [INFO ] W-9000-detectron2_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stderr
2021-03-23 17:42:48,009 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9000
2021-03-23 17:42:48,009 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13818
2021-03-23 17:42:48,009 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-23 17:42:48,009 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.9
2021-03-23 17:42:48,009 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-23 17:42:48,010 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2021-03-23 17:42:48,011 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9000.
2021-03-23 17:42:48,022 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-23 17:42:48,022 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-23 17:42:48,022 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 81, in load
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module, function_name = self._load_handler_file(handler)
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 115, in _load_handler_file
2021-03-23 17:42:48,023 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module = importlib.import_module(module_name)
2021-03-23 17:42:48,023 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/usr/lib/python3.6/importlib/__init__.py", line 126, in import_module
2021-03-23 17:42:48,023 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: detectron2, error: Worker died.
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     return _bootstrap._gcd_import(name[level:], package, level)
2021-03-23 17:42:48,023 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 994, in _gcd_import
2021-03-23 17:42:48,023 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stderr
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 971, in _find_and_load
2021-03-23 17:42:48,023 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stdout
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stdout
2021-03-23 17:42:48,023 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2021-03-23 17:42:48,024 [INFO ] W-9000-detectron2_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stderr
2021-03-23 17:42:51,093 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9000
2021-03-23 17:42:51,093 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13823
2021-03-23 17:42:51,093 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-23 17:42:51,093 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.9
2021-03-23 17:42:51,093 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2021-03-23 17:42:51,093 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2021-03-23 17:42:51,095 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9000.
2021-03-23 17:42:51,106 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
2021-03-23 17:42:51,106 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
2021-03-23 17:42:51,106 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-03-23 17:42:51,106 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 81, in load
2021-03-23 17:42:51,106 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module, function_name = self._load_handler_file(handler)
2021-03-23 17:42:51,106 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-03-23 17:42:51,106 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-03-23 17:42:51,106 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/home/linux-lyj/.local/lib/python3.6/site-packages/ts/model_loader.py", line 115, in _load_handler_file
2021-03-23 17:42:51,107 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: detectron2, error: Worker died.
2021-03-23 17:42:51,107 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module = importlib.import_module(module_name)
2021-03-23 17:42:51,107 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-03-23 17:42:51,107 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "/usr/lib/python3.6/importlib/__init__.py", line 126, in import_module
2021-03-23 17:42:51,107 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     return _bootstrap._gcd_import(name[level:], package, level)
2021-03-23 17:42:51,107 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stderr
2021-03-23 17:42:51,107 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 994, in _gcd_import
2021-03-23 17:42:51,107 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stdout
2021-03-23 17:42:51,107 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File "<frozen importlib._bootstrap>", line 971, in _find_and_load
2021-03-23 17:42:51,107 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2021-03-23 17:42:51,107 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stdout
2021-03-23 17:42:51,108 [INFO ] W-9000-detectron2_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stderr
2021-03-23 17:42:52,694 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-23 17:42:52,695 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-23 17:42:52,695 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-23 17:42:54,718 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-23 17:42:54,725 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model detectron2 version 1.0
2021-03-23 17:42:54,725 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: detectron2 version: 1.0
2021-03-23 17:42:54,726 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STOPPED -> WORKER_SCALED_DOWN
2021-03-23 17:42:54,727 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stderr
2021-03-23 17:42:54,727 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stdout
2021-03-23 17:42:54,828 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model detectron2 unregistered.
2021-03-23 17:45:55,989 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.3.0
TS Home: /home/linux-lyj/.local/lib/python3.8/site-packages
Current directory: /home/linux-lyj/git/torchserve
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 6
Max heap size: 3988 M
Python executable: /home/linux-lyj/anaconda3/envs/ts/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/linux-lyj/git/torchserve/model_store
Initial Models: detectron2=detectron2.mar
Log dir: /home/linux-lyj/git/torchserve/logs
Metrics dir: /home/linux-lyj/git/torchserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2021-03-23 17:45:56,014 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: detectron2.mar
2021-03-23 17:46:05,689 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag fe46ffdd65e046999d1178761b2d3971
2021-03-23 17:46:05,696 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model detectron2
2021-03-23 17:46:05,696 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model detectron2
2021-03-23 17:46:05,696 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model detectron2 loaded.
2021-03-23 17:46:05,696 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: detectron2, count: 1
2021-03-23 17:46:05,709 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2021-03-23 17:46:05,761 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2021-03-23 17:46:05,761 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2021-03-23 17:46:05,762 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2021-03-23 17:46:05,762 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2021-03-23 17:46:05,763 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2021-03-23 17:46:06,273 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9000
2021-03-23 17:46:06,277 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]13952
2021-03-23 17:46:06,277 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2021-03-23 17:46:06,278 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.8
2021-03-23 17:46:06,280 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change null -> WORKER_STARTED
2021-03-23 17:46:06,329 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2021-03-23 17:46:06,366 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9000.
2021-03-23 17:47:04,238 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - initializing starting
2021-03-23 17:47:04,238 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File model_final.pth exists True
2021-03-23 17:47:04,239 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File config.yaml exists True
2021-03-23 17:47:04,239 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - model_file completed.
2021-03-23 17:47:04,239 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - config_file completed.
2021-03-23 17:47:04,239 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - /tmp/models/fe46ffdd65e046999d1178761b2d3971/dt2_handler.py
2021-03-23 17:47:04,239 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Loading checkpoint from model_final.pth
2021-03-23 17:47:04,591 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - The checkpoint state_dict contains keys that are not used by the model:
2021-03-23 17:47:04,592 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   [35mpixel_mean[0m
2021-03-23 17:47:04,592 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   [35mpixel_std[0m
2021-03-23 17:47:04,609 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58151
2021-03-23 17:47:04,609 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2021-03-23 17:47:05,997 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - predictor built on initialize
2021-03-23 17:47:05,998 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - initialized
2021-03-23 17:47:05,998 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1388
2021-03-23 17:47:05,998 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - handling started
2021-03-23 17:47:05,998 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - pre-processing started for a batch of 1
2021-03-23 17:47:05,999 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - body
2021-03-23 17:47:06,000 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - pre-processing finished for a batch of 1
2021-03-23 17:47:06,000 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - inference started for a batch of 1
2021-03-23 17:47:06,001 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - inference finished for a batch of 1
2021-03-23 17:47:06,001 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.job.Job - Waiting time ns: 43068222750, Backend time ns: 1391333976
2021-03-23 17:47:06,001 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - post-processing started at 1616489225.9953346 for a batch of 1
2021-03-23 17:47:06,001 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - post-processing finished for a batch of 1 in 0.0011870861053466797
2021-03-23 17:47:06,002 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - handling finished
2021-03-23 17:47:11,579 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - handling started
2021-03-23 17:47:11,579 [INFO ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 326
2021-03-23 17:47:11,579 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - pre-processing started for a batch of 1
2021-03-23 17:47:11,579 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - body
2021-03-23 17:47:11,580 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - pre-processing finished for a batch of 1
2021-03-23 17:47:11,580 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - inference started for a batch of 1
2021-03-23 17:47:11,580 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - inference finished for a batch of 1
2021-03-23 17:47:11,580 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.job.Job - Waiting time ns: 114474, Backend time ns: 327724142
2021-03-23 17:47:11,580 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - post-processing started at 1616489231.5792031 for a batch of 1
2021-03-23 17:47:11,580 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - post-processing finished for a batch of 1 in 0.00014972686767578125
2021-03-23 17:47:11,580 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - handling finished
2021-03-23 17:47:23,471 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2021-03-23 17:47:23,471 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2021-03-23 17:47:23,471 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2021-03-23 17:47:25,493 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2021-03-23 17:47:25,499 [INFO ] Thread-0 org.pytorch.serve.ModelServer - Unregistering model detectron2 version 1.0
2021-03-23 17:47:25,499 [DEBUG] Thread-0 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: detectron2 version: 1.0
2021-03-23 17:47:25,500 [DEBUG] Thread-0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2021-03-23 17:47:25,500 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stderr
2021-03-23 17:47:25,501 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2021-03-23 17:47:25,502 [WARN ] W-9000-detectron2_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - /home/linux-lyj/anaconda3/envs/ts/lib/python3.8/site-packages/detectron2/modeling/roi_heads/fast_rcnn.py:154: UserWarning: This overload of nonzero is deprecated:
2021-03-23 17:47:25,502 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Frontend disconnected.
2021-03-23 17:47:25,502 [WARN ] Thread-0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stdout
2021-03-23 17:47:25,503 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2021-03-23 17:47:25,503 [INFO ] W-9000-detectron2_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stderr
2021-03-23 17:47:25,503 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2021-03-23 17:47:25,504 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-detectron2_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2021-03-23 17:47:25,505 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stderr
2021-03-23 17:47:25,505 [WARN ] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-detectron2_1.0-stdout
2021-03-23 17:47:25,505 [DEBUG] W-9000-detectron2_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2021-03-23 17:47:25,697 [INFO ] W-9000-detectron2_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-detectron2_1.0-stdout
2021-03-23 17:47:25,747 [INFO ] Thread-0 org.pytorch.serve.wlm.ModelManager - Model detectron2 unregistered.
